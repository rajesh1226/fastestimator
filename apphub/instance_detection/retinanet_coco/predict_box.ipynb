{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.architecture.retinanet import RetinaNet, get_fpn_anchor_box, get_target\n",
    "from fastestimator.dataset.mscoco import load_data\n",
    "from fastestimator.op import NumpyOp, TensorOp\n",
    "from fastestimator.op.numpyop import ImageReader, ResizeImageAndBbox, TypeConverter\n",
    "from fastestimator.op.tensorop import Loss, ModelOp, Pad, Rescale\n",
    "from fastestimator.trace import ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv, val_csv, path = load_data(path='/data/hsiming/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class String2List(NumpyOp):\n",
    "    # this thing converts '[1, 2, 3]' into np.array([1, 2, 3])\n",
    "    def forward(self, data, state):\n",
    "        data = map(literal_eval, data)\n",
    "        return data\n",
    "    \n",
    "\n",
    "class GenerateTarget(NumpyOp):\n",
    "    def __init__(self, inputs=None, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.anchorbox, _ = get_fpn_anchor_box(input_shape=(512, 512, 3))\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        obj_label, x1, y1, width, height = data\n",
    "        cls_gt, x1_gt, y1_gt, w_gt, h_gt = get_target(self.anchorbox, obj_label, x1, y1, width, height)\n",
    "        return cls_gt, x1_gt, y1_gt, w_gt, h_gt\n",
    "\n",
    "\n",
    "class RetinaLoss(Loss):\n",
    "    def focal_loss(self, cls_gt_example, cls_pred_example, alpha=0.25, gamma=2.0):\n",
    "        # cls_gt_example shape: [A], cls_pred_example shape: [A, K]\n",
    "        num_classes = cls_pred_example.shape[-1]\n",
    "        # gather the objects and background, discard the rest\n",
    "        anchor_obj_idx = tf.where(tf.greater_equal(cls_gt_example, 0))\n",
    "        anchor_obj_bg_idx = tf.where(tf.greater_equal(cls_gt_example, -1))\n",
    "        anchor_obj_count = tf.cast(tf.shape(anchor_obj_idx)[0], tf.float32)\n",
    "        cls_gt_example = tf.one_hot(cls_gt_example, num_classes)\n",
    "        cls_gt_example = tf.gather_nd(cls_gt_example, anchor_obj_bg_idx)\n",
    "        cls_pred_example = tf.gather_nd(cls_pred_example, anchor_obj_bg_idx)\n",
    "        cls_gt_example = tf.reshape(cls_gt_example, (-1, 1))\n",
    "        cls_pred_example = tf.reshape(cls_pred_example, (-1, 1))\n",
    "        # compute the focal weight on each selected anchor box\n",
    "        alpha_factor = tf.ones_like(cls_gt_example) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(cls_gt_example, 1), alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = tf.where(tf.equal(cls_gt_example, 1), 1 - cls_pred_example, cls_pred_example)\n",
    "        focal_weight = alpha_factor * focal_weight**gamma / anchor_obj_count\n",
    "        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(cls_gt_example,\n",
    "                                                                 cls_pred_example,\n",
    "                                                                 sample_weight=focal_weight)\n",
    "        return cls_loss, anchor_obj_idx\n",
    "\n",
    "    def smooth_l1(self, loc_gt_example, loc_pred_example, anchor_obj_idx, beta=0.1):\n",
    "        \"\"\"Return smooth l1 loss for box regesssion.\n",
    "\n",
    "        Args:\n",
    "            loc_gt_example (Tensor): Tensor of shape (padded=252, 4).\n",
    "            loc_pred_example (Tensor): Tensor of shape (num_anchors, 4).\n",
    "            anchor_obj_idx (Tensor): Indices of selected anchor box.\n",
    "\n",
    "        Returns:\n",
    "            float: Smooth l1 loss.\n",
    "        \"\"\"\n",
    "        loc_pred = tf.gather_nd(loc_pred_example, anchor_obj_idx)  #anchor_obj_count x 4\n",
    "        anchor_obj_count = tf.shape(loc_pred)[0]\n",
    "        loc_gt = loc_gt_example[:anchor_obj_count]  #anchor_obj_count x 4\n",
    "        loc_gt = tf.reshape(loc_gt, (-1, 1))\n",
    "        loc_pred = tf.reshape(loc_pred, (-1, 1))\n",
    "        loc_diff = tf.abs(loc_gt - loc_pred)\n",
    "        cond = tf.less(loc_diff, beta)\n",
    "        smooth_l1_loss = tf.where(cond, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n",
    "        smooth_l1_loss = tf.reduce_sum(smooth_l1_loss) / tf.cast(anchor_obj_count, tf.float32)\n",
    "        return smooth_l1_loss\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        cls_gt, x1_gt, y1_gt, w_gt, h_gt, cls_pred, loc_pred = data\n",
    "        local_batch_size = state[\"local_batch_size\"]\n",
    "        focal_loss = []\n",
    "        l1_loss = []\n",
    "        total_loss = []\n",
    "        for idx in range(local_batch_size):\n",
    "            cls_gt_example = cls_gt[idx]\n",
    "            x1_gt_example = x1_gt[idx]\n",
    "            y1_gt_example = y1_gt[idx]\n",
    "            w_gt_example = w_gt[idx]\n",
    "            h_gt_example = h_gt[idx]\n",
    "            loc_gt_example = tf.transpose(tf.stack([x1_gt_example, y1_gt_example, w_gt_example, h_gt_example]))\n",
    "            cls_pred_example = cls_pred[idx]\n",
    "            loc_pred_example = loc_pred[idx]\n",
    "            focal_loss_example, anchor_obj_idx = self.focal_loss(cls_gt_example, cls_pred_example)\n",
    "            smooth_l1_loss_example = self.smooth_l1(loc_gt_example, loc_pred_example, anchor_obj_idx)\n",
    "            focal_loss.append(focal_loss_example)\n",
    "            l1_loss.append(smooth_l1_loss_example)\n",
    "        focal_loss = tf.stack(focal_loss)\n",
    "        l1_loss = tf.stack(l1_loss)\n",
    "        total_loss = focal_loss + l1_loss\n",
    "\n",
    "        return total_loss, focal_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictBox(TensorOp):\n",
    "    def __init__(self,\n",
    "                 inputs=None,\n",
    "                 outputs=None,\n",
    "                 mode=None,\n",
    "                 input_shape=(512, 512, 3),\n",
    "                 select_top_k=1000,\n",
    "                 nms_max_outputs=100):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.input_shape = input_shape\n",
    "        self.select_top_k = tf.cast(select_top_k, dtype=tf.int32)\n",
    "        self.nms_max_outputs = nms_max_outputs\n",
    "\n",
    "        all_anchors, num_anchors_per_level = get_fpn_anchor_box(input_shape=input_shape)\n",
    "        self.all_anchors = tf.convert_to_tensor(all_anchors)\n",
    "        self.num_anchors_per_level = tf.convert_to_tensor(num_anchors_per_level, dtype=tf.int32)\n",
    "\n",
    "    def index_to_bool(self, indices, length):\n",
    "        updates = tf.ones_like(indices, dtype=tf.bool)\n",
    "        shape = tf.expand_dims(length, 0)\n",
    "        is_selected = tf.scatter_nd(tf.cast(tf.expand_dims(indices, axis=-1), dtype=tf.int32), updates, shape)\n",
    "        return is_selected\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        pred = []\n",
    "        gt = []\n",
    "\n",
    "        # extract max score and its class label\n",
    "        cls_pred, deltas, label_gt, x1_gt, y1_gt, w_gt, h_gt = data\n",
    "        labels = tf.cast(tf.argmax(cls_pred, axis=2), dtype=tf.int32)\n",
    "        scores = tf.reduce_max(cls_pred, axis=2)\n",
    "\n",
    "        # iterate over image\n",
    "        for i in range(state['local_batch_size']):\n",
    "            labels_per_image = labels[i]\n",
    "            scores_per_image = scores[i]\n",
    "            deltas_per_image = deltas[i]\n",
    "            \n",
    "            keep_gt = label_gt[i] > 0\n",
    "            label_gt_per_image = label_gt[i][keep_gt]\n",
    "            x1_gt_per_image = x1_gt[i][keep_gt]\n",
    "            y1_gt_per_image = y1_gt[i][keep_gt]\n",
    "            w_gt_per_image = w_gt[i][keep_gt]\n",
    "            h_gt_per_image = h_gt[i][keep_gt]\n",
    "\n",
    "            selected_deltas_per_image = tf.constant([], shape=(0, 4))\n",
    "            selected_labels_per_image = tf.constant([], dtype=tf.int32)\n",
    "            selected_scores_per_image = tf.constant([])\n",
    "            selected_anchor_indices_per_image = tf.constant([], dtype=tf.int32)\n",
    "\n",
    "            end_index = 0\n",
    "            # iterate over each pyramid level\n",
    "            for j in range(self.num_anchors_per_level.shape[0]):\n",
    "                start_index = end_index\n",
    "                end_index += self.num_anchors_per_level[j]\n",
    "                anchor_indices = tf.range(start_index, end_index, dtype=tf.int32)\n",
    "\n",
    "                level_scores = scores_per_image[start_index:end_index]\n",
    "                level_deltas = deltas_per_image[start_index:end_index]\n",
    "                level_labels = labels_per_image[start_index:end_index]\n",
    "\n",
    "                # select top k\n",
    "                if self.num_anchors_per_level[j] >= self.select_top_k:\n",
    "                    # won't work without the tf.minimum\n",
    "                    top_k = tf.math.top_k(level_scores, tf.minimum(self.num_anchors_per_level[j], self.select_top_k))\n",
    "                    top_k_scores = top_k.values\n",
    "                    top_k_indices = tf.add(top_k.indices, [start_index])\n",
    "                else:\n",
    "                    top_k_scores = level_scores\n",
    "                    top_k_indices = anchor_indices\n",
    "\n",
    "                # filter out low score\n",
    "                is_high_score = tf.greater(top_k_scores, 0.05)\n",
    "                selected_indices = tf.boolean_mask(top_k_indices, is_high_score)\n",
    "                is_selected = self.index_to_bool(tf.subtract(selected_indices, [start_index]),\n",
    "                                                 self.num_anchors_per_level[j])\n",
    "\n",
    "                # combine all pyramid levels\n",
    "                selected_deltas_per_image = tf.concat(\n",
    "                    [selected_deltas_per_image, tf.boolean_mask(level_deltas, is_selected)], axis=0)\n",
    "                selected_scores_per_image = tf.concat(\n",
    "                    [selected_scores_per_image, tf.boolean_mask(level_scores, is_selected)], axis=0)\n",
    "                selected_labels_per_image = tf.concat(\n",
    "                    [selected_labels_per_image, tf.boolean_mask(level_labels, is_selected)], axis=0)\n",
    "                selected_anchor_indices_per_image = tf.concat(\n",
    "                    [selected_anchor_indices_per_image, tf.boolean_mask(anchor_indices, is_selected)], axis=0)\n",
    "\n",
    "            # delta -> (x1, y1, w, h)\n",
    "            anchor_mask = self.index_to_bool(selected_anchor_indices_per_image, self.all_anchors.shape[0])\n",
    "            x1 = (selected_deltas_per_image[:, 0] * tf.boolean_mask(\n",
    "                self.all_anchors, anchor_mask)[:, 2]) + tf.boolean_mask(self.all_anchors, anchor_mask)[:, 0]\n",
    "            y1 = (selected_deltas_per_image[:, 1] * tf.boolean_mask(\n",
    "                self.all_anchors, anchor_mask)[:, 3]) + tf.boolean_mask(self.all_anchors, anchor_mask)[:, 1]\n",
    "            w = tf.math.exp(selected_deltas_per_image[:, 2]) * tf.boolean_mask(self.all_anchors, anchor_mask)[:, 2]\n",
    "            h = tf.math.exp(selected_deltas_per_image[:, 3]) * tf.boolean_mask(self.all_anchors, anchor_mask)[:, 3]\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "\n",
    "            # nms\n",
    "            boxes_per_image = tf.stack([y1, x1, y2, x2], axis=1)\n",
    "            nms_indices = tf.image.non_max_suppression(boxes_per_image, selected_scores_per_image, self.nms_max_outputs)\n",
    "\n",
    "            nms_boxes = tf.gather(boxes_per_image, nms_indices)\n",
    "            final_scores = tf.gather(selected_scores_per_image, nms_indices)\n",
    "            final_labels = tf.gather(selected_labels_per_image, nms_indices)\n",
    "\n",
    "            x1 = tf.clip_by_value(nms_boxes[:, 1], clip_value_min=0, clip_value_max=self.input_shape[1])\n",
    "            y1 = tf.clip_by_value(nms_boxes[:, 0], clip_value_min=0, clip_value_max=self.input_shape[0])\n",
    "            w = tf.clip_by_value(nms_boxes[:, 3], clip_value_min=0, clip_value_max=self.input_shape[1]) - x1\n",
    "            h = tf.clip_by_value(nms_boxes[:, 2], clip_value_min=0, clip_value_max=self.input_shape[0]) - y1\n",
    "\n",
    "            final_boxes = tf.stack([x1, y1, w, h], axis=1)\n",
    "\n",
    "            # combine image results into batch\n",
    "            image_results = tf.concat([\n",
    "                tf.pad(final_boxes, [[0, 0], [1, 0]], constant_values=i),\n",
    "                tf.cast(tf.expand_dims(final_labels, axis=1), dtype=tf.float32),\n",
    "                tf.expand_dims(final_scores, axis=1)\n",
    "            ],\n",
    "                                      axis=1)\n",
    "\n",
    "            image_gt = tf.transpose(\n",
    "               tf.concat([\n",
    "                   tf.stack([i * tf.ones_like(x1_gt_per_image), x1_gt_per_image]),\n",
    "                   tf.expand_dims(y1_gt_per_image, axis=0),\n",
    "                   tf.expand_dims(w_gt_per_image, axis=0),\n",
    "                   tf.expand_dims(h_gt_per_image, axis=0),\n",
    "                   tf.expand_dims(label_gt_per_image, axis=0)\n",
    "               ],\n",
    "                         axis=0))\n",
    "            pred.append(image_results)\n",
    "            gt.append(image_gt)\n",
    "            \n",
    "#             tf.print('image_gt', image_gt)\n",
    "#             tf.print('final_boxes', final_boxes)\n",
    "            \n",
    "        return tf.concat(pred, axis=0), tf.concat(gt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/data/hsiming/mscoco_model/'\n",
    "writer = fe.RecordWriter(\n",
    "    save_dir=os.path.join(path, \"retinanet_coco_all\"),\n",
    "    train_data='/data/hsiming/dataset/MSCOCO2017/train_object.csv',\n",
    "    validation_data='/data/hsiming/dataset/MSCOCO2017/val_object.csv',\n",
    "    ops=[\n",
    "        ImageReader(inputs=\"image\", parent_path=path, outputs=\"image\"),\n",
    "        String2List(inputs=[\"x1\", \"y1\", \"width\", \"height\", \"obj_label\"],\n",
    "                    outputs=[\"x1\", \"y1\", \"width\", \"height\", \"obj_label\"]),\n",
    "        ResizeImageAndBbox(target_size=(512, 512),\n",
    "                           keep_ratio=True,\n",
    "                           inputs=[\"image\", \"x1\", \"y1\", \"width\", \"height\"],\n",
    "                           outputs=[\"image\", \"x1\", \"y1\", \"width\", \"height\"]),\n",
    "        GenerateTarget(inputs=(\"obj_label\", \"x1\", \"y1\", \"width\", \"height\"),\n",
    "                       outputs=(\"cls_gt\", \"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\")),\n",
    "        TypeConverter(target_type='int32', inputs=[\"id\", \"cls_gt\"], outputs=[\"id\", \"cls_gt\"]),\n",
    "        TypeConverter(target_type='float32',\n",
    "                      inputs=[\"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\"],\n",
    "                      outputs=[\"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\"])\n",
    "    ],\n",
    "    compression=\"GZIP\",\n",
    "    write_feature=[\n",
    "        \"image\", \"id\", \"cls_gt\", \"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\", \"obj_label\", \"x1\", \"y1\", \"width\", \"height\"\n",
    "    ])\n",
    "\n",
    "# prepare pipeline\n",
    "pipeline = fe.Pipeline(\n",
    "    batch_size=8,\n",
    "    data=writer,\n",
    "    ops=[\n",
    "        Rescale(inputs=\"image\", outputs=\"image\"),\n",
    "        Pad(padded_shape=[190],\n",
    "            inputs=[\"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\", \"obj_label\", \"x1\", \"y1\", \"width\", \"height\"],\n",
    "            outputs=[\"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\", \"obj_label\", \"x1\", \"y1\", \"width\", \"height\"])\n",
    "    ])\n",
    "\n",
    "# prepare network\n",
    "model = fe.build(model_def=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n",
    "                 model_name=\"retinanet\",\n",
    "                 optimizer=tf.optimizers.Adam(learning_rate=0.0002),\n",
    "                 loss_name=\"total_loss\")\n",
    "network = fe.Network(ops=[\n",
    "    ModelOp(inputs=\"image\", model=model, outputs=[\"cls_pred\", \"loc_pred\"]),\n",
    "    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"obj_label\", \"x1\", \"y1\", \"width\", \"height\"],\n",
    "               outputs=(\"pred\", \"gt\"),\n",
    "               mode=\"eval\"),\n",
    "    RetinaLoss(inputs=(\"cls_gt\", \"x1_gt\", \"y1_gt\", \"w_gt\", \"h_gt\", \"cls_pred\", \"loc_pred\"),\n",
    "               outputs=(\"total_loss\", \"focal_loss\", \"l1_loss\"))\n",
    "])\n",
    "\n",
    "# prepare estimator\n",
    "estimator = fe.Estimator(\n",
    "    network=network,\n",
    "    pipeline=pipeline,\n",
    "    epochs=80,\n",
    "    #steps_per_epoch=2,\n",
    "    #log_steps=1,\n",
    "    #validation_steps=2,\n",
    "    traces=ModelSaver(model_name=\"retinanet\", save_dir=model_dir, save_best=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator: Saving tfrecord to /data/hsiming/dataset/MSCOCO2017/retinanet_coco_all\n",
      "FastEstimator: Converting Train TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 5.0%, Speed: 72.80 record/sec\n",
      "FastEstimator: Converting Train TFRecords 10.0%, Speed: 75.88 record/sec\n",
      "FastEstimator: Converting Train TFRecords 15.0%, Speed: 73.63 record/sec\n",
      "FastEstimator: Converting Train TFRecords 20.0%, Speed: 74.70 record/sec\n",
      "FastEstimator: Converting Train TFRecords 25.0%, Speed: 73.34 record/sec\n",
      "FastEstimator: Converting Train TFRecords 30.0%, Speed: 73.71 record/sec\n",
      "FastEstimator: Converting Train TFRecords 35.0%, Speed: 75.77 record/sec\n",
      "FastEstimator: Converting Train TFRecords 39.9%, Speed: 73.84 record/sec\n",
      "FastEstimator: Converting Train TFRecords 44.9%, Speed: 73.24 record/sec\n",
      "FastEstimator: Converting Train TFRecords 49.9%, Speed: 73.65 record/sec\n",
      "FastEstimator: Converting Train TFRecords 54.9%, Speed: 75.39 record/sec\n",
      "FastEstimator: Converting Train TFRecords 59.9%, Speed: 72.21 record/sec\n",
      "FastEstimator: Converting Train TFRecords 64.9%, Speed: 76.55 record/sec\n",
      "FastEstimator: Converting Train TFRecords 69.9%, Speed: 76.02 record/sec\n",
      "FastEstimator: Converting Train TFRecords 74.9%, Speed: 78.25 record/sec\n",
      "FastEstimator: Converting Train TFRecords 79.9%, Speed: 73.94 record/sec\n",
      "FastEstimator: Converting Train TFRecords 84.9%, Speed: 74.36 record/sec\n",
      "FastEstimator: Converting Train TFRecords 89.9%, Speed: 73.60 record/sec\n",
      "FastEstimator: Converting Train TFRecords 94.9%, Speed: 74.64 record/sec\n",
      "FastEstimator: Converting Train TFRecords 99.9%, Speed: 75.48 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 4.8%, Speed: 80.33 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 9.7%, Speed: 77.14 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 14.5%, Speed: 75.65 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 19.4%, Speed: 75.27 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 24.2%, Speed: 73.69 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 29.1%, Speed: 73.58 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 33.9%, Speed: 74.24 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 38.8%, Speed: 78.01 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 43.6%, Speed: 78.10 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 48.5%, Speed: 77.11 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 53.3%, Speed: 75.59 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 58.2%, Speed: 76.57 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 63.0%, Speed: 76.12 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 67.9%, Speed: 84.50 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 72.7%, Speed: 79.83 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 77.5%, Speed: 77.81 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 82.4%, Speed: 76.93 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 87.2%, Speed: 73.71 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 92.1%, Speed: 73.72 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 96.9%, Speed: 73.83 record/sec\n",
      "FastEstimator: Reading non-empty directory: /data/hsiming/dataset/MSCOCO2017/retinanet_coco_all\n",
      "FastEstimator: Found 117266 examples for train in /data/hsiming/dataset/MSCOCO2017/retinanet_coco_all/train_summary0.json\n",
      "FastEstimator: Found 4952 examples for eval in /data/hsiming/dataset/MSCOCO2017/retinanet_coco_all/eval_summary0.json\n",
      "FastEstimator-Start: step: 0; total_train_steps: 1172640; retinanet_lr: 0.0002; \n",
      "FastEstimator-Train: step: 0; focal_loss: 2.3346658; l1_loss: 0.9091846; total_loss: 3.2438502; \n",
      "FastEstimator-Train: step: 100; focal_loss: 1.090563; l1_loss: 0.6687578; total_loss: 1.7593209; examples/sec: 33.5; progress: 0.0%; \n",
      "FastEstimator-Train: step: 200; focal_loss: 1.1757247; l1_loss: 0.7640636; total_loss: 1.9397883; examples/sec: 33.3; progress: 0.0%; \n",
      "FastEstimator-Train: step: 300; focal_loss: 1.106729; l1_loss: 0.6396531; total_loss: 1.7463822; examples/sec: 33.3; progress: 0.0%; \n",
      "FastEstimator-Train: step: 400; focal_loss: 1.0742767; l1_loss: 0.747706; total_loss: 1.8219826; examples/sec: 33.3; progress: 0.0%; \n",
      "FastEstimator-Train: step: 500; focal_loss: 1.0397085; l1_loss: 0.6652808; total_loss: 1.7049894; examples/sec: 33.3; progress: 0.0%; \n",
      "FastEstimator-Train: step: 600; focal_loss: 1.0024981; l1_loss: 0.6254267; total_loss: 1.6279249; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 700; focal_loss: 0.9893917; l1_loss: 0.7206657; total_loss: 1.7100574; examples/sec: 33.3; progress: 0.1%; \n",
      "FastEstimator-Train: step: 800; focal_loss: 0.8354739; l1_loss: 0.6236014; total_loss: 1.4590753; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 900; focal_loss: 0.9089158; l1_loss: 0.5602871; total_loss: 1.4692028; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1000; focal_loss: 1.0786104; l1_loss: 0.6880257; total_loss: 1.7666363; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1100; focal_loss: 0.8220374; l1_loss: 0.642255; total_loss: 1.4642923; examples/sec: 33.3; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1200; focal_loss: 0.911554; l1_loss: 0.6343338; total_loss: 1.5458877; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1300; focal_loss: 0.8913521; l1_loss: 0.6084838; total_loss: 1.499836; examples/sec: 33.3; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1400; focal_loss: 0.9580761; l1_loss: 0.6042934; total_loss: 1.5623695; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1500; focal_loss: 0.9949554; l1_loss: 0.6351756; total_loss: 1.6301311; examples/sec: 33.3; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1600; focal_loss: 0.8266188; l1_loss: 0.6188284; total_loss: 1.4454473; examples/sec: 33.3; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1700; focal_loss: 0.8391511; l1_loss: 0.5672058; total_loss: 1.4063569; examples/sec: 33.2; progress: 0.1%; \n",
      "FastEstimator-Train: step: 1800; focal_loss: 0.9383421; l1_loss: 0.6645031; total_loss: 1.6028452; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 1900; focal_loss: 0.9015982; l1_loss: 0.686454; total_loss: 1.5880523; examples/sec: 33.2; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2000; focal_loss: 0.9577201; l1_loss: 0.5920639; total_loss: 1.549784; examples/sec: 33.2; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2100; focal_loss: 0.8429805; l1_loss: 0.5703432; total_loss: 1.4133236; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2200; focal_loss: 0.9946558; l1_loss: 0.3776542; total_loss: 1.3723099; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2300; focal_loss: 0.8079944; l1_loss: 0.5565552; total_loss: 1.3645496; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2400; focal_loss: 0.9106255; l1_loss: 0.5316944; total_loss: 1.4423199; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2500; focal_loss: 0.8019285; l1_loss: 0.596974; total_loss: 1.3989025; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2600; focal_loss: 0.8747607; l1_loss: 0.5877014; total_loss: 1.4624621; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2700; focal_loss: 0.7634298; l1_loss: 0.526156; total_loss: 1.2895858; examples/sec: 33.2; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2800; focal_loss: 0.8167908; l1_loss: 0.53443; total_loss: 1.3512207; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 2900; focal_loss: 0.789537; l1_loss: 0.4535075; total_loss: 1.2430446; examples/sec: 33.3; progress: 0.2%; \n",
      "FastEstimator-Train: step: 3000; focal_loss: 1.0069444; l1_loss: 0.6485359; total_loss: 1.6554804; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3100; focal_loss: 0.8731587; l1_loss: 0.5052716; total_loss: 1.3784304; examples/sec: 33.2; progress: 0.3%; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 3200; focal_loss: 0.8798137; l1_loss: 0.7627349; total_loss: 1.6425486; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3300; focal_loss: 0.8194082; l1_loss: 0.4866818; total_loss: 1.3060899; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3400; focal_loss: 0.795442; l1_loss: 0.5267133; total_loss: 1.3221552; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3500; focal_loss: 0.8672877; l1_loss: 0.5424381; total_loss: 1.4097258; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3600; focal_loss: 0.8193113; l1_loss: 0.3956013; total_loss: 1.2149127; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3700; focal_loss: 0.833902; l1_loss: 0.4563558; total_loss: 1.2902578; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3800; focal_loss: 0.8186311; l1_loss: 0.5018157; total_loss: 1.3204467; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 3900; focal_loss: 0.7698358; l1_loss: 0.4928996; total_loss: 1.2627354; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 4000; focal_loss: 0.9404283; l1_loss: 0.574248; total_loss: 1.5146762; examples/sec: 33.3; progress: 0.3%; \n",
      "FastEstimator-Train: step: 4100; focal_loss: 0.7183099; l1_loss: 0.5189424; total_loss: 1.2372522; examples/sec: 33.2; progress: 0.3%; \n",
      "FastEstimator-Train: step: 4200; focal_loss: 0.869963; l1_loss: 0.6387384; total_loss: 1.5087014; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4300; focal_loss: 0.7202796; l1_loss: 0.5163522; total_loss: 1.2366318; examples/sec: 33.2; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4400; focal_loss: 0.880412; l1_loss: 0.4894678; total_loss: 1.36988; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4500; focal_loss: 0.8852652; l1_loss: 0.4754946; total_loss: 1.3607599; examples/sec: 33.2; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4600; focal_loss: 0.8971257; l1_loss: 0.5128801; total_loss: 1.4100058; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4700; focal_loss: 0.7540058; l1_loss: 0.3967289; total_loss: 1.1507347; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4800; focal_loss: 0.8241012; l1_loss: 0.4524662; total_loss: 1.2765673; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 4900; focal_loss: 0.8442869; l1_loss: 0.54145; total_loss: 1.385737; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 5000; focal_loss: 0.8278202; l1_loss: 0.3562429; total_loss: 1.1840631; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 5100; focal_loss: 0.7187932; l1_loss: 0.4652496; total_loss: 1.1840429; examples/sec: 33.3; progress: 0.4%; \n",
      "FastEstimator-Train: step: 5200; focal_loss: 0.7574926; l1_loss: 0.4333206; total_loss: 1.1908132; examples/sec: 33.2; progress: 0.4%; \n",
      "FastEstimator-Train: step: 5300; focal_loss: 0.8192798; l1_loss: 0.5110165; total_loss: 1.3302963; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 5400; focal_loss: 0.7043516; l1_loss: 0.4018265; total_loss: 1.106178; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 5500; focal_loss: 0.7447735; l1_loss: 0.4735114; total_loss: 1.2182848; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 5600; focal_loss: 0.7082578; l1_loss: 0.6915269; total_loss: 1.3997847; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 5700; focal_loss: 0.8607037; l1_loss: 0.5608892; total_loss: 1.421593; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 5800; focal_loss: 0.7812298; l1_loss: 0.4687428; total_loss: 1.2499726; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 5900; focal_loss: 0.7933266; l1_loss: 0.4605442; total_loss: 1.2538707; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 6000; focal_loss: 0.7367841; l1_loss: 0.388822; total_loss: 1.1256062; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 6100; focal_loss: 0.6320628; l1_loss: 0.5032398; total_loss: 1.1353025; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 6200; focal_loss: 0.7217983; l1_loss: 0.4273316; total_loss: 1.1491297; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 6300; focal_loss: 0.8554608; l1_loss: 0.4202526; total_loss: 1.2757132; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 6400; focal_loss: 0.6824998; l1_loss: 0.5624557; total_loss: 1.2449555; examples/sec: 33.3; progress: 0.5%; \n",
      "FastEstimator-Train: step: 6500; focal_loss: 0.6170966; l1_loss: 0.3806475; total_loss: 0.9977442; examples/sec: 33.2; progress: 0.6%; \n",
      "FastEstimator-Train: step: 6600; focal_loss: 0.7665034; l1_loss: 0.4443333; total_loss: 1.2108366; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 6700; focal_loss: 0.6885268; l1_loss: 0.4262586; total_loss: 1.1147852; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 6800; focal_loss: 0.7866622; l1_loss: 0.4871838; total_loss: 1.273846; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 6900; focal_loss: 0.7114844; l1_loss: 0.450246; total_loss: 1.1617305; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7000; focal_loss: 0.7407956; l1_loss: 0.3967898; total_loss: 1.1375855; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7100; focal_loss: 0.6455134; l1_loss: 0.4463592; total_loss: 1.0918727; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7200; focal_loss: 0.6550972; l1_loss: 0.4431059; total_loss: 1.0982031; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7300; focal_loss: 0.7498611; l1_loss: 0.5164688; total_loss: 1.26633; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7400; focal_loss: 0.7150927; l1_loss: 0.5666018; total_loss: 1.2816944; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7500; focal_loss: 0.7802734; l1_loss: 0.6362512; total_loss: 1.4165246; examples/sec: 33.3; progress: 0.6%; \n",
      "FastEstimator-Train: step: 7600; focal_loss: 0.75504; l1_loss: 0.5424412; total_loss: 1.2974813; examples/sec: 33.3; progress: 0.6%; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
