{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Discriminative Domain Adaptation (ADDA) in FastEstimator\n",
    "\n",
    "In this notebook we will demonstrate how to perform domain adaptation in FastEstimator.\n",
    "Specifically, we will demonstrate one of adversarial training based domain adaptation methods, [*Adversarial Discriminative Domain Adaptation (ADDA)*](https://arxiv.org/abs/1702.05464). \n",
    "\n",
    "We will look at how to adapt a digit classifier trained on MNIST dataset to another digit dataset, USPS dataset.\n",
    "The digit classifer is composed of a feature extractor network and a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import fastestimator as fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Pipeline\n",
    "We will first download the two datasets using our dataset api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/fastestimator_data/USPS/zip.train.gz\n",
      "Extracting /root/fastestimator_data/USPS/zip.test.gz\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset import mnist, usps\n",
    "from fastestimator.op.numpyop import ImageReader\n",
    "from fastestimator import RecordWriter\n",
    "usps_train_csv, usps_eval_csv, usps_parent_dir = usps.load_data()\n",
    "mnist_train_csv, mnist_eval_csv, mnist_parent_dir = mnist.load_data()\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset api creates a train csv file with each row containing a relative path to a image and the class label.\n",
    "Two train csv files will have the same column names.\n",
    "We need to change these column names to unique name for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(mnist_train_csv)\n",
    "df.columns = ['source_img', 'source_label']\n",
    "df.to_csv(mnist_train_csv, index=False)\n",
    "\n",
    "df = pd.read_csv(usps_train_csv)\n",
    "df.columns = ['target_img', 'target_label']\n",
    "df.to_csv(usps_train_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the modified csv files, we can now create an input data pipeline that returns a batch from the MNIST dataset and the USPS dataset. \n",
    "#### Note that the input data pipeline created here is an unpaired dataset of the MNIST and the USPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import Resize, Minmax\n",
    "\n",
    "writer = RecordWriter(save_dir=os.path.join(os.path.dirname(mnist_parent_dir), 'adda', 'tfr'),\n",
    "                      train_data=(usps_train_csv, mnist_train_csv),\n",
    "                      ops=(\n",
    "                          [ImageReader(inputs=\"target_img\", outputs=\"target_img\", parent_path=usps_parent_dir, grey_scale=True)], # first tuple element\n",
    "                          [ImageReader(inputs=\"source_img\", outputs=\"source_img\", parent_path=mnist_parent_dir, grey_scale=True)])) # second tuple element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the following preprocessing to both datasets:\n",
    "* Resize of images to $32\\times32$\n",
    "* Minmax pixel value normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Saving tfrecord to /root/fastestimator_data/adda/tfr\n",
      "FastEstimator: Converting Train TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 5.0%, Speed: 6438.19 record/sec\n",
      "FastEstimator: Converting Train TFRecords 10.0%, Speed: 9156.87 record/sec\n",
      "FastEstimator: Converting Train TFRecords 15.0%, Speed: 10721.43 record/sec\n",
      "FastEstimator: Converting Train TFRecords 20.0%, Speed: 10055.73 record/sec\n",
      "FastEstimator: Converting Train TFRecords 25.0%, Speed: 8862.50 record/sec\n",
      "FastEstimator: Converting Train TFRecords 30.0%, Speed: 9326.50 record/sec\n",
      "FastEstimator: Converting Train TFRecords 34.9%, Speed: 8585.62 record/sec\n",
      "FastEstimator: Converting Train TFRecords 39.9%, Speed: 8430.13 record/sec\n",
      "FastEstimator: Converting Train TFRecords 44.9%, Speed: 8324.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 49.9%, Speed: 8462.68 record/sec\n",
      "FastEstimator: Converting Train TFRecords 54.9%, Speed: 8178.13 record/sec\n",
      "FastEstimator: Converting Train TFRecords 59.9%, Speed: 8283.31 record/sec\n",
      "FastEstimator: Converting Train TFRecords 64.9%, Speed: 8706.65 record/sec\n",
      "FastEstimator: Converting Train TFRecords 69.9%, Speed: 9099.43 record/sec\n",
      "FastEstimator: Converting Train TFRecords 74.9%, Speed: 9475.19 record/sec\n",
      "FastEstimator: Converting Train TFRecords 79.9%, Speed: 9820.01 record/sec\n",
      "FastEstimator: Converting Train TFRecords 84.9%, Speed: 10149.50 record/sec\n",
      "FastEstimator: Converting Train TFRecords 89.9%, Speed: 10459.54 record/sec\n",
      "FastEstimator: Converting Train TFRecords 94.8%, Speed: 10757.69 record/sec\n",
      "FastEstimator: Converting Train TFRecords 99.8%, Speed: 11044.58 record/sec\n",
      "FastEstimator: Converting Train TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 5.0%, Speed: 11054.88 record/sec\n",
      "FastEstimator: Converting Train TFRecords 10.0%, Speed: 12287.17 record/sec\n",
      "FastEstimator: Converting Train TFRecords 15.0%, Speed: 10449.15 record/sec\n",
      "FastEstimator: Converting Train TFRecords 20.0%, Speed: 9954.79 record/sec\n",
      "FastEstimator: Converting Train TFRecords 25.0%, Speed: 10100.48 record/sec\n",
      "FastEstimator: Converting Train TFRecords 30.0%, Speed: 9998.55 record/sec\n",
      "FastEstimator: Converting Train TFRecords 35.0%, Speed: 10080.56 record/sec\n",
      "FastEstimator: Converting Train TFRecords 40.0%, Speed: 10618.81 record/sec\n",
      "FastEstimator: Converting Train TFRecords 45.0%, Speed: 10902.46 record/sec\n",
      "FastEstimator: Converting Train TFRecords 50.0%, Speed: 11026.90 record/sec\n",
      "FastEstimator: Converting Train TFRecords 55.0%, Speed: 10960.67 record/sec\n",
      "FastEstimator: Converting Train TFRecords 60.0%, Speed: 11302.76 record/sec\n",
      "FastEstimator: Converting Train TFRecords 65.0%, Speed: 11152.31 record/sec\n",
      "FastEstimator: Converting Train TFRecords 70.0%, Speed: 10863.13 record/sec\n",
      "FastEstimator: Converting Train TFRecords 75.0%, Speed: 10893.36 record/sec\n",
      "FastEstimator: Converting Train TFRecords 80.0%, Speed: 11142.46 record/sec\n",
      "FastEstimator: Converting Train TFRecords 85.0%, Speed: 11371.03 record/sec\n",
      "FastEstimator: Converting Train TFRecords 90.0%, Speed: 11583.05 record/sec\n",
      "FastEstimator: Converting Train TFRecords 95.0%, Speed: 11784.00 record/sec\n",
      "FastEstimator: Reading non-empty directory: /root/fastestimator_data/adda/tfr\n",
      "FastEstimator: Found 60000 examples for train in /root/fastestimator_data/adda/tfr/train_summary1.json\n",
      "FastEstimator: Found 7291 examples for train in /root/fastestimator_data/adda/tfr/train_summary0.json\n"
     ]
    }
   ],
   "source": [
    "pipeline = fe.Pipeline(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    data=writer,\n",
    "    ops=[\n",
    "        Resize(inputs=\"target_img\", outputs=\"target_img\", size=(32, 32)),\n",
    "        Resize(inputs=\"source_img\", outputs=\"source_img\", size=(32, 32)),\n",
    "        Minmax(inputs=\"target_img\", outputs=\"target_img\"),\n",
    "        Minmax(inputs=\"source_img\", outputs=\"source_img\")\n",
    "    ]\n",
    ")\n",
    "a = pipeline.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize an example output from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.squeeze(a[0][\"source_img\"][1]), cmap='gray');\n",
    "plt.axis('off');\n",
    "plt.title('Sample MNIST Image');\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.squeeze(a[0][\"target_img\"][3]), cmap='gray');\n",
    "plt.axis('off');\n",
    "plt.title('Sample USPS Image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "\n",
    "With ``Pipeline`` defined, we define the network architecture.\n",
    "As we dicussed previously, the classification model is composed of the feature extraction network and the classifier network.\n",
    "The training scheme is very similar to that of GAN; the objective is to train a feature extractor network for the USPS dataset so that the discriminator cannot reliably distinguish MNIST examples and USPS examples.\n",
    "The feature extractor network for the USPS dataset is initialized from the feature extractor network for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Sequential\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), \"feature_extractor.h5\")\n",
    "\n",
    "def build_feature_extractor(input_shape=(32, 32, 1), feature_dim=512):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(feature_dim, activation='relu'))        \n",
    "    return model\n",
    "\n",
    "def build_classifer(feature_dim=512, num_classes=10):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', input_dim=feature_dim))\n",
    "    return model\n",
    "\n",
    "def build_discriminator(feature_dim=512):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(1024, activation='relu', input_dim=feature_dim))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Step2: Define Network\n",
    "feature_extractor = fe.build(model_def=build_feature_extractor,\n",
    "                             model_name=\"fe\",\n",
    "                             loss_name=\"fe_loss\",\n",
    "                             optimizer=tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9))\n",
    "\n",
    "discriminator = fe.build(model_def=build_discriminator,\n",
    "                         model_name=\"disc\",\n",
    "                         loss_name=\"d_loss\",\n",
    "                         optimizer=tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a ``TensorOp`` to extract a feature from MNIST images.\n",
    "This feature will be used as an input to the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op import TensorOp\n",
    "from fastestimator.op.tensorop import Loss, ModelOp\n",
    "\n",
    "class ExtractSourceFeature(TensorOp):\n",
    "    def __init__(self, model_path, inputs, outputs=None, mode=None):\n",
    "        super().__init__(inputs, outputs, mode)\n",
    "        self.source_feature_extractor = tf.keras.models.load_model(model_path, compile=False)\n",
    "        self.source_feature_extractor.trainable = False\n",
    "\n",
    "    def forward(self, data, state):        \n",
    "        return self.source_feature_extractor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define loss functions for the feature extractor network and the discriminator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FELoss(Loss):\n",
    "    def __init__(self, inputs, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "        \n",
    "    def forward(self, data, state):\n",
    "        target_score = data        \n",
    "        return self.cross_entropy(tf.ones_like(target_score), target_score)\n",
    "\n",
    "class DLoss(Loss):\n",
    "    def __init__(self, inputs, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "        \n",
    "    def forward(self, data, state):\n",
    "        source_score, target_score = data\n",
    "        source_loss = self.cross_entropy(tf.ones_like(source_score), source_score)\n",
    "        target_loss = self.cross_entropy(tf.zeros_like(target_score), target_score)\n",
    "        total_loss = source_loss + target_loss\n",
    "        return 0.5 * total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the forward pass of the networks within one iteration of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(inputs=\"target_img\", outputs=\"target_feature\", model=feature_extractor),\n",
    "    ModelOp(inputs=\"target_feature\", outputs=\"target_score\", model=discriminator),\n",
    "    ExtractSourceFeature(model_path=model_path, inputs=\"source_img\", outputs=\"source_feature\"),\n",
    "    ModelOp(inputs=\"source_feature\", outputs=\"source_score\", model=discriminator),\n",
    "    DLoss(inputs=(\"source_score\", \"target_score\"), outputs=\"d_loss\"),\n",
    "    FELoss(inputs=\"target_score\", outputs=\"fe_loss\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define two ``Trace``:\n",
    "* ``LoadPretrainedFE`` to load the weights of the feature extractor trained on MNIST\n",
    "* ``EvaluateTargetClassifier`` to evaluate the classifier on the USPS dataset.\n",
    "\n",
    "There are three key thins to keep in mind:\n",
    "* The classifier network is never updated with any target label information\n",
    "* Only the feature extractor is fine tuned to confuse the discriminator network\n",
    "* The classifier only classifies on the basis of the output of the feature extractor network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.trace import Trace\n",
    "\n",
    "class LoadPretrainedFE(Trace):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def on_begin(self, state):\n",
    "        self.network.model[self.model_name].load_weights('feature_extractor.h5')\n",
    "        print(\"FastEstimator-LoadPretrainedFE: loaded pretrained feature extractor\")\n",
    "\n",
    "class EvaluateTargetClassifier(Trace):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.target_model = tf.keras.Sequential()\n",
    "        self.acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "    def on_begin(self, state):\n",
    "        self.target_model.add(self.network.model[self.model_name])\n",
    "        self.target_model.add(\n",
    "            tf.keras.models.load_model(\"classifier.h5\")\n",
    "        )\n",
    "    def on_batch_end(self, state):\n",
    "        if state[\"epoch\"] == 0 or state[\"epoch\"] == 99:\n",
    "            target_img, target_label = state[\"batch\"][\"target_img\"], state[\"batch\"][\"target_label\"]\n",
    "            logits = self.target_model(target_img)\n",
    "            self.acc_metric(target_label, logits)\n",
    "    \n",
    "    def on_epoch_end(self, state):\n",
    "        if state[\"epoch\"] == 0 or state[\"epoch\"] == 99:\n",
    "            acc = self.acc_metric.result()\n",
    "            print(\"FastEstimator-EvaluateTargetClassifier: %0.4f at epoch %d\" % (acc, state[\"epoch\"]))\n",
    "            self.acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Estimator\n",
    "\n",
    "With ``Pipeline``, ``Network``, and ``Trace`` defined, we now define ``Estimator`` to put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [LoadPretrainedFE(model_name=\"fe\"),\n",
    "          EvaluateTargetClassifier(model_name=\"fe\")]\n",
    "estimator = fe.Estimator(\n",
    "    pipeline= pipeline, \n",
    "    network=network,\n",
    "    traces = traces,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call ``fit`` method to start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator: Reading non-empty directory: /root/fastestimator_data/adda/tfr\n",
      "FastEstimator: Found 60000 examples for train in /root/fastestimator_data/adda/tfr/train_summary1.json\n",
      "FastEstimator: Found 7291 examples for train in /root/fastestimator_data/adda/tfr/train_summary0.json\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-LoadPretrainedFE: loaded pretrained feature extractor\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "FastEstimator-Start: step: 0; total_train_steps: 5600; fe_lr: 1e-04; disc_lr: 1e-04; \n",
      "FastEstimator-Train: step: 0; fe_loss: 0.6089596; d_loss: 0.681249; \n",
      "FastEstimator-EvaluateTargetClassifier: 0.7874 at epoch 0\n",
      "FastEstimator-Train: step: 100; fe_loss: 1.0726796; d_loss: 0.4771829; examples/sec: 5513.1; progress: 1.8%; \n",
      "FastEstimator-Train: step: 200; fe_loss: 1.1022232; d_loss: 0.4764578; examples/sec: 6433.9; progress: 3.6%; \n",
      "FastEstimator-Train: step: 300; fe_loss: 1.2459202; d_loss: 0.4798668; examples/sec: 6221.2; progress: 5.4%; \n",
      "FastEstimator-Train: step: 400; fe_loss: 1.3324102; d_loss: 0.4748652; examples/sec: 6457.0; progress: 7.1%; \n",
      "FastEstimator-Train: step: 500; fe_loss: 1.5858265; d_loss: 0.5034922; examples/sec: 3345.9; progress: 8.9%; \n",
      "FastEstimator-Train: step: 600; fe_loss: 0.7085456; d_loss: 0.5749392; examples/sec: 6861.5; progress: 10.7%; \n",
      "FastEstimator-Train: step: 700; fe_loss: 1.7074094; d_loss: 0.5349842; examples/sec: 6895.7; progress: 12.5%; \n",
      "FastEstimator-Train: step: 800; fe_loss: 1.5799985; d_loss: 0.482881; examples/sec: 6817.5; progress: 14.3%; \n",
      "FastEstimator-Train: step: 900; fe_loss: 0.9060917; d_loss: 0.5182171; examples/sec: 8033.4; progress: 16.1%; \n",
      "FastEstimator-Train: step: 1000; fe_loss: 1.0265226; d_loss: 0.4402258; examples/sec: 2913.7; progress: 17.9%; \n",
      "FastEstimator-Train: step: 1100; fe_loss: 0.9295922; d_loss: 0.494996; examples/sec: 6929.2; progress: 19.6%; \n",
      "FastEstimator-Train: step: 1200; fe_loss: 1.339618; d_loss: 0.5340438; examples/sec: 6848.2; progress: 21.4%; \n",
      "FastEstimator-Train: step: 1300; fe_loss: 0.5638706; d_loss: 0.59447; examples/sec: 7950.7; progress: 23.2%; \n",
      "FastEstimator-Train: step: 1400; fe_loss: 1.5171843; d_loss: 0.5671506; examples/sec: 6750.7; progress: 25.0%; \n",
      "FastEstimator-Train: step: 1500; fe_loss: 1.52879; d_loss: 0.4776084; examples/sec: 3130.6; progress: 26.8%; \n",
      "FastEstimator-Train: step: 1600; fe_loss: 1.1557212; d_loss: 0.5874342; examples/sec: 6832.0; progress: 28.6%; \n",
      "FastEstimator-Train: step: 1700; fe_loss: 0.7100754; d_loss: 0.5629431; examples/sec: 7975.3; progress: 30.4%; \n",
      "FastEstimator-Train: step: 1800; fe_loss: 1.2991563; d_loss: 0.5433497; examples/sec: 6720.4; progress: 32.1%; \n",
      "FastEstimator-Train: step: 1900; fe_loss: 0.4490385; d_loss: 0.7224292; examples/sec: 2871.2; progress: 33.9%; \n",
      "FastEstimator-Train: step: 2000; fe_loss: 1.1373507; d_loss: 0.5484131; examples/sec: 6895.8; progress: 35.7%; \n",
      "FastEstimator-Train: step: 2100; fe_loss: 1.0352225; d_loss: 0.5430945; examples/sec: 7992.6; progress: 37.5%; \n",
      "FastEstimator-Train: step: 2200; fe_loss: 1.522444; d_loss: 0.5251599; examples/sec: 6872.3; progress: 39.3%; \n",
      "FastEstimator-Train: step: 2300; fe_loss: 1.4009509; d_loss: 0.508774; examples/sec: 6809.2; progress: 41.1%; \n",
      "FastEstimator-Train: step: 2400; fe_loss: 1.5312254; d_loss: 0.5388322; examples/sec: 2896.9; progress: 42.9%; \n",
      "FastEstimator-Train: step: 2500; fe_loss: 1.6519206; d_loss: 0.570011; examples/sec: 8011.6; progress: 44.6%; \n",
      "FastEstimator-Train: step: 2600; fe_loss: 0.9349663; d_loss: 0.5017464; examples/sec: 6938.2; progress: 46.4%; \n",
      "FastEstimator-Train: step: 2700; fe_loss: 1.1607829; d_loss: 0.4954276; examples/sec: 6897.4; progress: 48.2%; \n",
      "FastEstimator-Train: step: 2800; fe_loss: 1.1784583; d_loss: 0.4916884; examples/sec: 6905.6; progress: 50.0%; \n",
      "FastEstimator-Train: step: 2900; fe_loss: 1.2136245; d_loss: 0.5028952; examples/sec: 3363.6; progress: 51.8%; \n",
      "FastEstimator-Train: step: 3000; fe_loss: 0.9984823; d_loss: 0.5247761; examples/sec: 6869.3; progress: 53.6%; \n",
      "FastEstimator-Train: step: 3100; fe_loss: 0.9749998; d_loss: 0.496695; examples/sec: 6826.1; progress: 55.4%; \n",
      "FastEstimator-Train: step: 3200; fe_loss: 0.9823316; d_loss: 0.4926038; examples/sec: 6785.6; progress: 57.1%; \n",
      "FastEstimator-Train: step: 3300; fe_loss: 1.6716573; d_loss: 0.5352554; examples/sec: 3120.0; progress: 58.9%; \n",
      "FastEstimator-Train: step: 3400; fe_loss: 1.0688219; d_loss: 0.5558775; examples/sec: 6889.5; progress: 60.7%; \n",
      "FastEstimator-Train: step: 3500; fe_loss: 1.2807276; d_loss: 0.5147618; examples/sec: 6859.0; progress: 62.5%; \n",
      "FastEstimator-Train: step: 3600; fe_loss: 1.3385832; d_loss: 0.5068592; examples/sec: 6872.5; progress: 64.3%; \n",
      "FastEstimator-Train: step: 3700; fe_loss: 1.4010513; d_loss: 0.5224268; examples/sec: 8247.9; progress: 66.1%; \n",
      "FastEstimator-Train: step: 3800; fe_loss: 1.0652959; d_loss: 0.5546493; examples/sec: 3066.8; progress: 67.9%; \n",
      "FastEstimator-Train: step: 3900; fe_loss: 0.9134641; d_loss: 0.5320688; examples/sec: 6864.7; progress: 69.6%; \n",
      "FastEstimator-Train: step: 4000; fe_loss: 0.7933622; d_loss: 0.5298726; examples/sec: 6904.5; progress: 71.4%; \n",
      "FastEstimator-Train: step: 4100; fe_loss: 1.0647731; d_loss: 0.5019788; examples/sec: 6809.6; progress: 73.2%; \n",
      "FastEstimator-Train: step: 4200; fe_loss: 1.4380388; d_loss: 0.5365941; examples/sec: 8224.2; progress: 75.0%; \n",
      "FastEstimator-Train: step: 4300; fe_loss: 1.7495753; d_loss: 0.6115906; examples/sec: 3139.0; progress: 76.8%; \n",
      "FastEstimator-Train: step: 4400; fe_loss: 1.1284056; d_loss: 0.4732235; examples/sec: 6933.8; progress: 78.6%; \n",
      "FastEstimator-Train: step: 4500; fe_loss: 0.9639484; d_loss: 0.5253327; examples/sec: 6829.3; progress: 80.4%; \n",
      "FastEstimator-Train: step: 4600; fe_loss: 0.9848607; d_loss: 0.464295; examples/sec: 8281.9; progress: 82.1%; \n",
      "FastEstimator-Train: step: 4700; fe_loss: 0.8769747; d_loss: 0.4915569; examples/sec: 2876.2; progress: 83.9%; \n",
      "FastEstimator-Train: step: 4800; fe_loss: 1.7685512; d_loss: 0.4833464; examples/sec: 6717.0; progress: 85.7%; \n",
      "FastEstimator-Train: step: 4900; fe_loss: 1.7158316; d_loss: 0.5014951; examples/sec: 6947.6; progress: 87.5%; \n",
      "FastEstimator-Train: step: 5000; fe_loss: 1.1106799; d_loss: 0.4509357; examples/sec: 8318.1; progress: 89.3%; \n",
      "FastEstimator-Train: step: 5100; fe_loss: 1.0473665; d_loss: 0.4548874; examples/sec: 4789.3; progress: 91.1%; \n",
      "FastEstimator-Train: step: 5200; fe_loss: 0.9096103; d_loss: 0.4814547; examples/sec: 1765.7; progress: 92.9%; \n",
      "FastEstimator-Train: step: 5300; fe_loss: 1.0935009; d_loss: 0.5313207; examples/sec: 6374.3; progress: 94.6%; \n",
      "FastEstimator-Train: step: 5400; fe_loss: 0.9431607; d_loss: 0.5197348; examples/sec: 7953.5; progress: 96.4%; \n",
      "FastEstimator-Train: step: 5500; fe_loss: 0.4644586; d_loss: 0.6450016; examples/sec: 6866.9; progress: 98.2%; \n",
      "FastEstimator-EvaluateTargetClassifier: 0.9402 at epoch 99\n",
      "FastEstimator-Finish: step: 5600; total_time: 134.23 sec; fe_lr: 1e-04; disc_lr: 1e-04; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``EvaluateTargetClassifier`` outputs the classification accuracy on the USPS at the beginning of the training and the end of the training. We can observe significant improvement in the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
