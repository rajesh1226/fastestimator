{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, models\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.op import TensorOp\n",
    "from fastestimator.op.numpyop import Minmax\n",
    "from fastestimator.trace.io import ModelSaver\n",
    "from fastestimator.util.compute_overlap import compute_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 512\n",
    "HEIGHT = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SVHN data set\n",
    "\n",
    "#### Once this is done we will use COCO data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset import svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_csv, test_csv, path = svhn.load_data('/home/ubuntu/SVHN/')\n",
    "path = '/home/ubuntu/SVHN'\n",
    "train_csv = os.path.join(path,'train.csv')\n",
    "test_csv = os.path.join(path, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.png</td>\n",
       "      <td>[1, 9]</td>\n",
       "      <td>[246, 323]</td>\n",
       "      <td>[77, 81]</td>\n",
       "      <td>[327, 419]</td>\n",
       "      <td>[296, 300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.png</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[77, 98]</td>\n",
       "      <td>[29, 25]</td>\n",
       "      <td>[100, 124]</td>\n",
       "      <td>[61, 57]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.png</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>[17, 25]</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[25, 34]</td>\n",
       "      <td>[20, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.png</td>\n",
       "      <td>[9, 3]</td>\n",
       "      <td>[57, 72]</td>\n",
       "      <td>[13, 13]</td>\n",
       "      <td>[72, 85]</td>\n",
       "      <td>[47, 47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.png</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[52, 74]</td>\n",
       "      <td>[7, 10]</td>\n",
       "      <td>[73, 89]</td>\n",
       "      <td>[53, 56]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image   label          x1        y1          x2          y2\n",
       "0  train/1.png  [1, 9]  [246, 323]  [77, 81]  [327, 419]  [296, 300]\n",
       "1  train/2.png  [2, 3]    [77, 98]  [29, 25]  [100, 124]    [61, 57]\n",
       "2  train/3.png  [2, 5]    [17, 25]    [5, 5]    [25, 34]    [20, 20]\n",
       "3  train/4.png  [9, 3]    [57, 72]  [13, 13]    [72, 85]    [47, 47]\n",
       "4  train/5.png  [3, 1]    [52, 74]   [7, 10]    [73, 89]    [53, 56]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a smaller data set\n",
    "\n",
    "#### TFRecords generation at high resolution (800x1280) is super slow. I am creating a smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_csv)\n",
    "test = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = train[0:3000]\n",
    "test_small = test[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot image and bounding box\n",
    "#### Note: I found that 30.png, 18522.png are not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image    train/4001.png\n",
       "label            [6, 1]\n",
       "x1             [23, 38]\n",
       "y1               [3, 4]\n",
       "x2             [37, 45]\n",
       "y2             [28, 29]\n",
       "Name: 4000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = cv2.imread(os.path.join(path, df['image'][index]))\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "fig, ax = plt.subplots()\n",
    "for box_index in range(len(literal_eval(df['label'][index]))):\n",
    "    x1 = literal_eval(df['x1'][index])[box_index]\n",
    "    y1 = literal_eval(df['y1'][index])[box_index]\n",
    "    x2 = literal_eval(df['x2'][index])[box_index]\n",
    "    y2 = literal_eval(df['y2'][index])[box_index]\n",
    "    cv2.rectangle(im, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "    \n",
    "ax.imshow(im)\n",
    "df.loc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfrecord writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op import NumpyOp\n",
    "from fastestimator.op.numpyop import ImageReader, Minmax, Resize\n",
    "from fastestimator.util import RecordWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class String2List(NumpyOp):\n",
    "    # this thing converts '[1, 2, 3]' into np.array([1, 2, 3])\n",
    "    def forward(self, data, state):\n",
    "        data = map(literal_eval, data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeCocoStyle(Resize):\n",
    "    def __init__(self, target_size, keep_ratio=False, inputs=None, outputs=None, mode=None):\n",
    "        super().__init__(target_size, keep_ratio=keep_ratio, inputs=inputs, outputs=outputs, mode=mode)\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        img, x1, y1, x2, y2 = data\n",
    "        if self.keep_ratio:\n",
    "            original_ratio = img.shape[1] / img.shape[0]\n",
    "            target_ratio = self.target_size[1] / self.target_size[0]\n",
    "            if original_ratio >= target_ratio:\n",
    "                pad = (img.shape[1] / target_ratio - img.shape[0]) / 2\n",
    "                pad_boarder = (np.ceil(pad).astype(np.int), np.floor(pad).astype(np.int), 0, 0)\n",
    "                y1 += np.ceil(pad).astype(np.int)\n",
    "                y2 += np.ceil(pad).astype(np.int)\n",
    "            else:\n",
    "                pad = (img.shape[0] * target_ratio - img.shape[1]) / 2\n",
    "                pad_boarder = (0, 0, np.ceil(pad).astype(np.int), np.floor(pad).astype(np.int))\n",
    "                x1 += np.ceil(pad).astype(np.int)\n",
    "                x2 += np.ceil(pad).astype(np.int)\n",
    "\n",
    "            img = cv2.copyMakeBorder(img, *pad_boarder, cv2.BORDER_CONSTANT)\n",
    "            \n",
    "        img_resize = cv2.resize(img, (self.target_size[1], self.target_size[0]), self.resize_method)\n",
    "        x1 = (np.array(x1) * self.target_size[1] / img.shape[1]).astype(np.int64)\n",
    "        x2 = (np.array(x2) * self.target_size[1] / img.shape[1]).astype(np.int64)\n",
    "        y1 = (np.array(y1) * self.target_size[0] / img.shape[0]).astype(np.int64)\n",
    "        y2 = (np.array(y2) * self.target_size[0] / img.shape[0]).astype(np.int64)\n",
    "        return img_resize, x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateTarget(NumpyOp):\n",
    "    def __init__(self, inputs=None, outputs=None, mode=None, input_shape=(800, 800, 3)):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        self.sizes = [32, 64, 128, 256, 512]\n",
    "        self.strides = [8, 16, 32, 64, 128]\n",
    "        self.ratios = np.array([0.5, 1, 2], dtype=np.float)\n",
    "        self.scales = np.array([2**0, 2**(1.0 / 3.0), 2**(2.0 / 3.0)], dtype=np.float)\n",
    "\n",
    "        self.anchors_list = np.zeros((0, 4))\n",
    "        image_shapes = [(np.array(input_shape[:2]) + 2**pyra_level - 1) // (2**pyra_level)\n",
    "                        for pyra_level in self.pyramid_levels]\n",
    "        for idx, pyra_level in enumerate(self.pyramid_levels):\n",
    "            base_size = self.sizes[idx]\n",
    "            ratios = self.ratios\n",
    "            scales = self.scales\n",
    "            image_shape = image_shapes[idx]\n",
    "            strides = self.strides[idx]\n",
    "            anchors = self.generate_anchors_core(base_size, ratios, scales)\n",
    "            shifted_anchors = self.shift(image_shape, strides, anchors)\n",
    "            self.anchors_list = np.append(self.anchors_list, shifted_anchors, axis=0)\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        label, x1, y1, x2, y2, image = data\n",
    "        target_cls, target_loc = self.get_target(self.anchors_list, label, x1, y1, x2, y2, num_classes=11) # 10 classes + bg\n",
    "        return target_cls, target_loc, self.anchors_list\n",
    "\n",
    "    def get_target(self, anchorbox, label, x1, y1, x2, y2, num_classes=10):\n",
    "        bg_index = num_classes - 1\n",
    "        query_box = np.zeros((0, 4))\n",
    "        query_label = np.zeros((0))\n",
    "        for _x1, _y1, _x2, _y2, _label in zip(x1, y1, x2, y2, label):\n",
    "            query_box = np.append(query_box, np.array([[_x1, _y1, _x2, _y2]]), axis=0)\n",
    "            query_label = np.append(query_label, _label)\n",
    "\n",
    "        overlap = compute_overlap(anchorbox.astype(np.float64), query_box.astype(np.float64))\n",
    "        argmax_overlaps_inds = np.argmax(overlap, axis=1)\n",
    "        max_overlaps = overlap[np.arange(overlap.shape[0]), argmax_overlaps_inds]\n",
    "        positive_index = (max_overlaps > 0.5)\n",
    "        ignore_index = (max_overlaps > 0.4) & ~positive_index\n",
    "        negative_index = (max_overlaps <= 0.4)\n",
    "\n",
    "        target_loc = get_loc_offset(query_box[argmax_overlaps_inds, :], anchorbox)\n",
    "        target_cls = query_label[argmax_overlaps_inds]\n",
    "        target_cls[negative_index] = bg_index\n",
    "        target_cls[ignore_index] = -2  # ignore this example\n",
    "\n",
    "        return target_cls, target_loc\n",
    "\n",
    "    def generate_anchors_core(self, base_size, ratios, scales):\n",
    "        num_anchors = len(ratios) * len(scales)\n",
    "        # initialize output anchors\n",
    "        anchors = np.zeros((num_anchors, 4))\n",
    "        # scale base_size\n",
    "        anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "        # compute areas of anchors\n",
    "        areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "        # correct for ratios\n",
    "        anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "        anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "        # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "        anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "        anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def shift(self, image_shape, stride, anchors):\n",
    "        shift_x = (np.arange(0, image_shape[1]) + 0.5) * stride\n",
    "        shift_y = (np.arange(0, image_shape[0]) + 0.5) * stride\n",
    "        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n",
    "\n",
    "        A = anchors.shape[0]\n",
    "        K = shifts.shape[0]\n",
    "        all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "        all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "        return all_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_offset(box_gt, box_anchor):\n",
    "    mean = 0\n",
    "    std = 0.2\n",
    "    anchor_width_height = np.tile(box_anchor[:, 2:] - box_anchor[:, :2], [1, 2])\n",
    "    delta = (box_gt - box_anchor) / anchor_width_height\n",
    "    return delta / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = RecordWriter(\n",
    "    train_data=os.path.join(path, 'train_small.csv'),\n",
    "    validation_data=os.path.join(path, 'test_small.csv'),\n",
    "    save_dir=os.path.join(path, 'tfrecords'),\n",
    "    ops=[\n",
    "        ImageReader(inputs='image', outputs='image', parent_path=path),\n",
    "        String2List(inputs=[\"label\", \"x1\", \"y1\", \"x2\", \"y2\"], outputs=[\"label\", \"x1\", \"y1\", \"x2\", \"y2\"]),\n",
    "        ResizeCocoStyle((HEIGHT, WIDTH),\n",
    "                        keep_ratio=True,\n",
    "                        inputs=[\"image\", \"x1\", \"y1\", \"x2\", \"y2\"],\n",
    "                        outputs=[\"image\", \"x1\", \"y1\", \"x2\", \"y2\"]),\n",
    "        Minmax(inputs=\"image\", outputs=\"image\"),\n",
    "        GenerateTarget(inputs=[\"label\", \"x1\", \"y1\", \"x2\", \"y2\", \"image\"],\n",
    "                       outputs=[\"target_cls\", \"target_loc\", \"base_loc\"],\n",
    "                       input_shape=(HEIGHT, WIDTH, 3))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(data=writer, batch_size=2, padded_batch=True, read_feature=[\"image\", \"target_cls\", \"target_loc\",\"base_loc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/ubuntu/SVHN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Reading non-empty directory: /home/ubuntu/SVHN/tfrecords\n",
      "FastEstimator: Found 5000 examples for train in /home/ubuntu/SVHN/tfrecords/train_summary0.json\n",
      "FastEstimator: Found 3000 examples for eval in /home/ubuntu/SVHN/tfrecords/eval_summary0.json\n"
     ]
    }
   ],
   "source": [
    "batch_data = pipeline.show_results(mode='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([800, 1280, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0]['image'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'target_cls', 'target_loc', 'base_loc'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=264, shape=(191970, 4), dtype=float32, numpy=\n",
       "array([[ -18.627417 ,   -7.3137083,   26.627417 ,   15.313708 ],\n",
       "       [ -24.508759 ,  -10.254379 ,   32.50876  ,   18.25438  ],\n",
       "       [ -31.918785 ,  -13.959393 ,   39.918785 ,   21.959393 ],\n",
       "       ...,\n",
       "       [1034.9807   ,  469.96133  , 1397.0193   , 1194.0387   ],\n",
       "       [ 987.92993  ,  375.85986  , 1444.0701   , 1288.1401   ],\n",
       "       [ 928.6497   ,  257.29944  , 1503.3503   , 1406.7006   ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0]['base_loc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f10e51e1748>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_index = 0\n",
    "im = batch_data[0]['image'].numpy()[batch_index]\n",
    "fig, ax = plt.subplots()\n",
    "# for box_index in range(len(batch_data[0]['x1'].numpy()[batch_index])):\n",
    "#     x1 = batch_data[0]['x1'].numpy()[batch_index][box_index]\n",
    "#     y1 = batch_data[0]['y1'].numpy()[batch_index][box_index]\n",
    "#     x2 = batch_data[0]['x2'].numpy()[batch_index][box_index]\n",
    "#     y2 = batch_data[0]['y2'].numpy()[batch_index][box_index]\n",
    "#     if x1 > 0:\n",
    "#         im = cv2.rectangle(im, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "ax.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras definitions\n",
    "\n",
    "#### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_sub_net(num_classes, num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(num_classes * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='sigmoid',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer=tf.initializers.constant(np.log(1 / 99))))\n",
    "    model.add(layers.Reshape((-1, num_classes)))  # the output dimension is [batch, #anchor, #classes]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_sub_net(num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(4 * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(layers.Reshape((-1, 4)))  # the output dimension is [batch, #anchor, 4]\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RetinaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetinaNet(input_shape, num_classes, num_anchor=9):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    # FPN\n",
    "    #     weights = '/home/ubuntu/ResNet-50-model.keras.h5'\n",
    "    resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)\n",
    "    #     resnet50.load_weights(weights, by_name=True)\n",
    "    assert resnet50.layers[80].name == \"conv3_block4_out\"\n",
    "    C3 = resnet50.layers[80].output\n",
    "    assert resnet50.layers[142].name == \"conv4_block6_out\"\n",
    "    C4 = resnet50.layers[142].output\n",
    "    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n",
    "    C5 = resnet50.layers[-1].output\n",
    "    \n",
    "    P5 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same')(C5)\n",
    "    P5_upsampling = layers.UpSampling2D()(P5)\n",
    "    P4 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same')(C4)\n",
    "    P4 = layers.Add()([P5_upsampling, P4])\n",
    "    P4_upsampling = layers.UpSampling2D()(P4)\n",
    "    P3 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same')(C3)\n",
    "    \n",
    "    P3 = layers.Add()([P4_upsampling, P3])\n",
    "    \n",
    "    P6 = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', name=\"P6\")(C5)\n",
    "    P7 = layers.Activation('relu')(P6)\n",
    "    P7 = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', name=\"P7\")(P7)\n",
    "    P5 = layers.Conv2D(256, kernel_size=3, strides=1, padding='same', name=\"P5\")(P5)\n",
    "    P4 = layers.Conv2D(256, kernel_size=3, strides=1, padding='same', name=\"P4\")(P4)\n",
    "    P3 = layers.Conv2D(256, kernel_size=3, strides=1, padding='same', name=\"P3\")(P3)\n",
    "    # classification subnet\n",
    "    cls_subnet = classification_sub_net(num_classes=num_classes, num_anchor=num_anchor)\n",
    "    P3_cls = cls_subnet(P3)\n",
    "    P4_cls = cls_subnet(P4)\n",
    "    P5_cls = cls_subnet(P5)\n",
    "    P6_cls = cls_subnet(P6)\n",
    "    P7_cls = cls_subnet(P7)\n",
    "    cls_output = layers.Concatenate(axis=-2)([P3_cls, P4_cls, P5_cls, P6_cls, P7_cls])\n",
    "    # localization subnet\n",
    "    loc_subnet = regression_sub_net(num_anchor=num_anchor)\n",
    "    P3_loc = loc_subnet(P3)\n",
    "    P4_loc = loc_subnet(P4)\n",
    "    P5_loc = loc_subnet(P5)\n",
    "    P6_loc = loc_subnet(P6)\n",
    "    P7_loc = loc_subnet(P7)\n",
    "    loc_output = layers.Concatenate(axis=-2)([P3_loc, P4_loc, P5_loc, P6_loc, P7_loc])\n",
    "    return tf.keras.Model(inputs=inputs, outputs=[cls_output, loc_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaLoss(Loss):\n",
    "    def focal_loss(self, cls_gt_example, cls_pred_example, num_classes, alpha=0.25, gamma=2.0):\n",
    "        bg_index = num_classes - 1\n",
    "        # cls_gt has shape [A], cls_pred is in [A, K]\n",
    "        # gather the objects and background, discard the rest\n",
    "        obj_idx = tf.where(tf.logical_and(tf.greater_equal(cls_gt_example, 0), tf.less(cls_gt_example, bg_index)))\n",
    "        obj_bg_idx = tf.where(tf.greater_equal(cls_gt_example, 0))\n",
    "        obj_bg_count = tf.cast(tf.shape(obj_bg_idx)[0], tf.float32)\n",
    "        obj_count = tf.cast(tf.maximum(tf.shape(obj_idx)[0], 1), tf.float32)\n",
    "        cls_gt_example = tf.one_hot(cls_gt_example, num_classes)\n",
    "        cls_gt_example = tf.gather_nd(cls_gt_example, obj_bg_idx)\n",
    "        cls_pred_example = tf.gather_nd(cls_pred_example, obj_bg_idx)\n",
    "        cls_gt_example = tf.reshape(cls_gt_example, (-1, 1))\n",
    "        cls_pred_example = tf.reshape(cls_pred_example, (-1, 1))\n",
    "        # compute the focal weight on each selected anchor box\n",
    "        alpha_factor = tf.ones_like(cls_gt_example) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(cls_gt_example, 1), alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = tf.where(tf.equal(cls_gt_example, 1), 1 - cls_pred_example, cls_pred_example)\n",
    "        focal_weight = alpha_factor * focal_weight**gamma / obj_count\n",
    "        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(cls_gt_example,\n",
    "                                                                 cls_pred_example,\n",
    "                                                                 sample_weight=focal_weight)\n",
    "        return cls_loss, obj_idx\n",
    "\n",
    "    def smooth_l1(self, loc_gt_example, loc_pred_example, obj_idx):\n",
    "        # loc_gt anf loc_pred has shape [A, 4]\n",
    "        sigma = 3\n",
    "        sigma_squared = sigma**3\n",
    "        obj_count = tf.cast(tf.maximum(tf.shape(obj_idx)[0], 1), tf.float32)\n",
    "        loc_gt = tf.gather_nd(loc_gt_example, obj_idx)\n",
    "        loc_pred = tf.gather_nd(loc_pred_example, obj_idx)\n",
    "        loc_gt = tf.reshape(loc_gt, (-1, 1))\n",
    "        loc_pred = tf.reshape(loc_pred, (-1, 1))\n",
    "        loc_diff = tf.abs(loc_gt - loc_pred)\n",
    "        smooth_l1_loss = tf.where(tf.less(loc_diff, 1 / sigma_squared),\n",
    "                                  0.5 * loc_diff**2 * sigma_squared,\n",
    "                                  loc_diff - 0.5 / sigma_squared)\n",
    "        smooth_l1_loss = tf.reduce_sum(smooth_l1_loss) / obj_count\n",
    "        return smooth_l1_loss\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        cls_gt, loc_gt, cls_pred, loc_pred = data\n",
    "        cls_gt = tf.cast(cls_gt, tf.int32)\n",
    "        batch_size = state[\"local_batch_size\"]\n",
    "        total_loss = []\n",
    "        for idx in range(batch_size):\n",
    "            cls_gt_example = cls_gt[idx]\n",
    "            loc_gt_example = loc_gt[idx]\n",
    "            cls_pred_example = cls_pred[idx]\n",
    "            loc_pred_example = loc_pred[idx]\n",
    "            focal_loss, obj_idx = self.focal_loss(cls_gt_example, cls_pred_example, num_classes=10+1) # 10 classes + bg\n",
    "            smooth_l1_loss = self.smooth_l1(loc_gt_example, loc_pred_example, obj_idx)\n",
    "            total_loss.append(focal_loss + smooth_l1_loss)\n",
    "        total_loss = tf.convert_to_tensor(total_loss)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not using PredictBox yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictBox(TensorOp):\n",
    "    def __init__(self, num_classes, inputs=None, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.num_classes = num_classes\n",
    "        self.bg_index = num_classes - 1\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        cls_pred, loc_pred, loc_base = data\n",
    "        input_width = 1280\n",
    "        input_height = 800\n",
    "        top_n = 300\n",
    "        score_threshold = 0.05\n",
    "        std = 0.2\n",
    "        mean = 0\n",
    "        # convert the residual prediction to absolute prediction in (x1, y1, x2, y2)\n",
    "        anchor_w_h = tf.tile(loc_base[:, :, 2:], [1, 1, 2]) - tf.tile(loc_base[:, :, :2], [1, 1, 2])\n",
    "        anchorbox = loc_base\n",
    "        loc_pred_abs = tf.map_fn(lambda x: (x[0] * std + mean) * x[1] + x[2],\n",
    "                                 elems=(loc_pred, anchor_w_h, anchorbox),\n",
    "                                 dtype=tf.float32,\n",
    "                                 back_prop=False)\n",
    "        x1 = tf.clip_by_value(loc_pred_abs[:, :, 0], 0, input_width)\n",
    "        y1 = tf.clip_by_value(loc_pred_abs[:, :, 1], 0, input_height)\n",
    "        x2 = tf.clip_by_value(loc_pred_abs[:, :, 2], 0, input_width)\n",
    "        y2 = tf.clip_by_value(loc_pred_abs[:, :, 3], 0, input_height)\n",
    "        loc_pred_abs = tf.stack([x1, y1, x2, y2], axis=2)\n",
    "\n",
    "        num_batch, num_anchor, _ = loc_pred_abs.shape\n",
    "        cls_best_score = tf.reduce_max(cls_pred, axis=-1)\n",
    "        cls_best_class = tf.argmax(cls_pred, axis=-1)\n",
    "\n",
    "        cls_best_score = tf.where(tf.not_equal(cls_best_class, self.bg_index), cls_best_score, 0)\n",
    "\n",
    "        # select top n anchor boxes to proceed\n",
    "        # Padded Nonmax suppression with threshold\n",
    "        selected_indices_padded = tf.map_fn(\n",
    "            lambda x: tf.image.non_max_suppression_padded(\n",
    "                x[0], x[1], top_n, pad_to_max_output_size=True, score_threshold=score_threshold).selected_indices,\n",
    "            (loc_pred_abs, cls_best_score),\n",
    "            dtype=tf.int32,\n",
    "            back_prop=False)\n",
    "        valid_outputs = tf.map_fn(\n",
    "            lambda x: tf.image.non_max_suppression_padded(\n",
    "                x[0], x[1], top_n, pad_to_max_output_size=True, score_threshold=score_threshold).valid_outputs,\n",
    "            (loc_pred_abs, cls_best_score),\n",
    "            dtype=tf.int32,\n",
    "            back_prop=False)\n",
    "        return loc_pred_abs, selected_indices_padded, valid_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop.model import ModelOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "#10 + 1 comess from 10 classes plus bg class\n",
    "model = fe.build(model_def=lambda: RetinaNet(input_shape=(HEIGHT, WIDTH, 3), num_classes=10 + 1),\n",
    "                 model_name=\"retinanet\",\n",
    "                 optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                 loss_name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(inputs=\"image\", model=model, outputs=[\"pred_cls\", \"pred_loc\"]),\n",
    "    RetinaLoss(inputs=(\"target_cls\", \"target_loc\", \"pred_cls\", \"pred_loc\"), outputs=\"loss\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastesti2",
   "language": "python",
   "name": "fastesti2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
