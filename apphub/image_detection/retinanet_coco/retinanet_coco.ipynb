{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import tempfile\n",
    "import tarfile\n",
    "from zipfile import ZipFile\n",
    "from glob import glob \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.estimator.estimator import Estimator\n",
    "from fastestimator.network.loss import Loss\n",
    "from fastestimator.network.model import ModelOp, FEModel\n",
    "from fastestimator.network.network import Network\n",
    "from fastestimator.pipeline.pipeline import Pipeline\n",
    "from fastestimator.pipeline.processing import Minmax\n",
    "from fastestimator.record.preprocess import ImageReader, Resize\n",
    "from fastestimator.record.record import RecordWriter\n",
    "from fastestimator.util.op import NumpyOp, TensorOp\n",
    "from fastestimator.estimator.trace import MeanAveragePrecision, LRController, ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.util.compute_overlap import compute_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, models\n",
    "\n",
    "def classification_sub_net(num_classes, num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01), bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01), bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01), bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01), bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(num_classes * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='sigmoid',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer=tf.initializers.constant(np.log(1 / 99))))\n",
    "    model.add(layers.Reshape((-1, num_classes)))  # the output dimension is [batch, #anchor, #classes]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_sub_net(num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros')) \n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(\n",
    "        layers.Conv2D(4 * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer='zeros'))\n",
    "    model.add(layers.Reshape((-1, 4)))  # the output dimension is [batch, #anchor, 4]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetinaNet(input_shape, num_classes, num_anchor=9):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    # FPN\n",
    "#     weights = '/home/ubuntu/ResNet-50-model.keras.h5'\n",
    "    resnet50 = tf.keras.applications.ResNet50(weights= \"imagenet\", include_top=False, input_tensor=inputs, pooling=None)\n",
    "#     resnet50.load_weights(weights, by_name=True)\n",
    "    assert resnet50.layers[80].name == \"conv3_block4_out\"\n",
    "    C3 = resnet50.layers[80].output\n",
    "    assert resnet50.layers[142].name == \"conv4_block6_out\"\n",
    "    C4 = resnet50.layers[142].output\n",
    "    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n",
    "    C5 = resnet50.layers[-1].output\n",
    "    P5 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same')(C5)\n",
    "    P5_upsampling = layers.UpSampling2D()(P5)\n",
    "    P4 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same')(C4)\n",
    "    P4 = layers.Add()([P5_upsampling, P4])\n",
    "    P4_upsampling = layers.UpSampling2D()(P4)\n",
    "    P3 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same')(C3)\n",
    "    P3 = layers.Add()([P4_upsampling, P3])\n",
    "    P6 = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', name=\"P6\")(C5)\n",
    "    P7 = layers.Activation('relu')(P6)\n",
    "    P7 = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', name=\"P7\")(P7)\n",
    "    P5 = layers.Conv2D(256, kernel_size=3, strides=1, padding='same', name=\"P5\")(P5)\n",
    "    P4 = layers.Conv2D(256, kernel_size=3, strides=1, padding='same', name=\"P4\")(P4)\n",
    "    P3 = layers.Conv2D(256, kernel_size=3, strides=1, padding='same', name=\"P3\")(P3)\n",
    "    # classification subnet\n",
    "    cls_subnet = classification_sub_net(num_classes=num_classes, num_anchor=num_anchor)\n",
    "    P3_cls = cls_subnet(P3)\n",
    "    P4_cls = cls_subnet(P4)\n",
    "    P5_cls = cls_subnet(P5)\n",
    "    P6_cls = cls_subnet(P6)\n",
    "    P7_cls = cls_subnet(P7)\n",
    "    cls_output = layers.Concatenate(axis=-2)([P3_cls, P4_cls, P5_cls, P6_cls, P7_cls])\n",
    "    # localization subnet\n",
    "    loc_subnet = regression_sub_net(num_anchor=num_anchor)\n",
    "    P3_loc = loc_subnet(P3)\n",
    "    P4_loc = loc_subnet(P4)\n",
    "    P5_loc = loc_subnet(P5)\n",
    "    P6_loc = loc_subnet(P6)\n",
    "    P7_loc = loc_subnet(P7)\n",
    "    loc_output = layers.Concatenate(axis=-2)([P3_loc, P4_loc, P5_loc, P6_loc, P7_loc])\n",
    "    return tf.keras.Model(inputs=inputs, outputs=[cls_output, loc_output])\n",
    "\n",
    "\n",
    "\n",
    "# def get_loc_offset(box_gt, box_anchor):\n",
    "#     mean = 0 \n",
    "#     std = 0.2\n",
    "#     gt_x1, gt_y1, gt_x2, gt_y2 = tuple(box_gt)\n",
    "#     ac_x1, ac_y1, ac_x2, ac_y2 = tuple(box_anchor)\n",
    "#     anchor_width = ac_x2 - ac_x1\n",
    "#     anchor_height = ac_y2 - ac_y1\n",
    "#     dx1 = (gt_x1 - ac_x1) / anchor_width\n",
    "#     dx1 = dx1 / std\n",
    "#     dy1 = (gt_y1 - ac_y1) / anchor_height\n",
    "#     dy1 = dy1 / std\n",
    "#     dx2 = (gt_x2 - ac_x2) / anchor_width\n",
    "#     dx2 = dx2 / std\n",
    "#     dy2 = (gt_y2 - ac_y2) / anchor_height\n",
    "#     dy2 = dy2 /std\n",
    "#     return dx1, dy1, dx2, dy2\n",
    "\n",
    "\n",
    "def get_loc_offset(box_gt, box_anchor):\n",
    "    mean = 0 \n",
    "    std = 0.2\n",
    "    anchor_width_height = np.tile(box_anchor[:,2:] - box_anchor[:,:2],[1,2])\n",
    "    delta =  (box_gt - box_anchor)/ anchor_width_height\n",
    "    return delta/std\n",
    "\n",
    "\n",
    "def get_iou(box1, box2):\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = tuple(box1)\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = tuple(box2)\n",
    "    xA = max(b1_x1, b2_x1)\n",
    "    yA = max(b1_y1, b2_y1)\n",
    "    xB = min(b1_x2, b2_x2)\n",
    "    yB = min(b1_y2, b2_y2)\n",
    "    \n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    if interArea == 0:\n",
    "        iou = 0\n",
    "    else:\n",
    "        box1Area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "        box2Area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "        iou = interArea / (box1Area + box2Area - interArea)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "# SHAPE=(288*2,288*2,3)\n",
    "\n",
    "class ResnetPreprocess(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        image = data\n",
    "        image = image.astype(np.float64)\n",
    "        image[..., 0] -= 103.939\n",
    "        image[..., 1] -= 116.779\n",
    "        image[..., 2] -= 123.68\n",
    "        return image        \n",
    "\n",
    "class String2FloatArray(NumpyOp):\n",
    "    # this thing converts '[1, 2, 3]' into np.array([1, 2, 3])\n",
    "    def forward(self, data, state):\n",
    "        for idx, elem in enumerate(data):\n",
    "            elem = literal_eval(elem)\n",
    "            data[idx] = np.array(elem, dtype=np.int32)\n",
    "#             data[idx] = np.array([float(x) for x in elem[1:-1].split(',')])\n",
    "        return data\n",
    "    \n",
    "class String2IntArray(NumpyOp):\n",
    "    # this thing converts '[1, 2, 3]' into np.array([1, 2, 3])\n",
    "    def forward(self, data, state):\n",
    "        for idx, elem in enumerate(data):\n",
    "            elem = literal_eval(elem)\n",
    "            data[idx] = np.array(elem ,dtype=np.int32)\n",
    "#             data[idx] = np.array([int(x) for x in elem[1:-1].split(',')])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "class RelativeCoordinate(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        image, x1, y1, x2, y2 = data\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        x1, y1, x2, y2 = x1 / width, y1 / height, x2 / width, y2 / height\n",
    "        return x1, y1, x2, y2\n",
    "    \n",
    "class ImageAdjustedBatchMax(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        images = data\n",
    "        batch_size = len(images)\n",
    "        max_shape = tuple(max(image.shape[idx] for image in images) for idx in range(3) )\n",
    "        images_max_shape = np.zeros((batch_size,)+max_shape)\n",
    "        for id, image in enumerate(images):\n",
    "            images_max_shape[idx, :image.shape[0], :image.shape[1], :image.shape[2] ] = image\n",
    "        return image_max_shape\n",
    "    \n",
    "class ResizeCocoStyle(Resize):\n",
    "    def __init__(self, target_size, keep_ratio=False, inputs=None, outputs=None, mode=None):\n",
    "        super().__init__(target_size, keep_ratio=keep_ratio, inputs=inputs, outputs=outputs, mode=mode)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, state):\n",
    "        img, x1, y1, x2, y2 = data \n",
    "        if self.keep_ratio:\n",
    "            original_ratio = img.shape[1] / img.shape[0]\n",
    "            target_ratio = self.target_size[1] / self.target_size[0]\n",
    "            if original_ratio >= target_ratio:\n",
    "                pad = (img.shape[1] / target_ratio - img.shape[0]) / 2\n",
    "                pad_boarder = (np.ceil(pad).astype(np.int), np.floor(pad).astype(np.int), 0, 0)\n",
    "                y1 += np.ceil(pad).astype(np.int)\n",
    "                y2 += np.ceil(pad).astype(np.int)\n",
    "            else:\n",
    "                pad = (img.shape[0] * target_ratio - img.shape[1]) / 2\n",
    "                pad_boarder = (0, 0, np.ceil(pad).astype(np.int), np.floor(pad).astype(np.int))\n",
    "                x1 += np.ceil(pad).astype(np.int)\n",
    "                x2 += np.ceil(pad).astype(np.int)\n",
    "                \n",
    "            img = self._cv2.copyMakeBorder(img, *pad_boarder, self._cv2.BORDER_CONSTANT)\n",
    "        img_resize = self._cv2.resize(img, (self.target_size[1], self.target_size[0]), self.resize_method)\n",
    "        x1 = x1 * self.target_size[1]/img.shape[1]\n",
    "        x2 = x2 * self.target_size[1]/img.shape[1]\n",
    "        y1 = y1 * self.target_size[0]/img.shape[0]\n",
    "        y2 = y2 * self.target_size[0]/img.shape[0]\n",
    "        return img_resize, x1,y1,x2,y2\n",
    "        \n",
    "            \n",
    "    \n",
    "class GenerateTarget(NumpyOp):\n",
    "    def __init__(self, inputs=None, outputs=None, mode=None, input_shape=(800,800,3)):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.pyramid_levels = [3,4,5,6,7]\n",
    "        self.sizes   = [32, 64, 128, 256, 512]\n",
    "        self.strides = [8, 16, 32, 64, 128]\n",
    "        self.ratios  = np.array([0.5, 1, 2], dtype=np.float)\n",
    "        self.scales  = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)], dtype=np.float)\n",
    "        \n",
    "        \n",
    "        self.anchors_list = np.zeros((0,4))\n",
    "        image_shapes = [(np.array(input_shape[:2]) + 2**pyra_level-1)//(2**pyra_level) for pyra_level in self.pyramid_levels]\n",
    "        for idx, pyra_level in enumerate(self.pyramid_levels) :\n",
    "            base_size=self.sizes[idx]\n",
    "            ratios=self.ratios\n",
    "            scales=self.scales\n",
    "            image_shape = image_shapes[idx]\n",
    "            strides = self.strides[idx]\n",
    "            anchors = self.generate_anchors_core(base_size, ratios, scales)\n",
    "            shifted_anchors = self.shift(image_shape, strides, anchors)\n",
    "            self.anchors_list = np.append(self.anchors_list, shifted_anchors, axis=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, state):\n",
    "        label, x1, y1, x2, y2, image = data\n",
    "        target_cls, target_loc = self.get_target(self.anchors_list, label, x1, y1, x2, y2, num_classes=81)\n",
    "        return target_cls, target_loc, self.anchors_list\n",
    "        \n",
    "        \n",
    "#     def get_target(self, anchorbox, label, x1, y1, x2, y2, num_classes=20):\n",
    "#         bg_index = num_classes - 1\n",
    "#         num_anchor = anchorbox.shape[0]\n",
    "#         target_cls = np.zeros(shape=(num_anchor), dtype=np.int64) + (bg_index)  #initializing with bg_index\n",
    "#         target_loc = np.zeros(shape=(num_anchor, 4), dtype=np.float32)\n",
    "#         target_iou = np.zeros(shape=(num_anchor), dtype=np.float32)\n",
    "#         for _label, _x1, _y1, _x2, _y2 in zip(label, x1, y1, x2, y2):\n",
    "#             best_iou = 0.0\n",
    "#             for anchor_idx in range(num_anchor):\n",
    "#                 iou = get_iou((_x1, _y1, _x2, _y2), anchorbox[anchor_idx])\n",
    "\n",
    "#                 if iou > best_iou:\n",
    "#                     best_iou = iou\n",
    "#                     best_anchor_idx = anchor_idx\n",
    "#                 if iou > target_iou[anchor_idx]:    \n",
    "#                     if iou > 0.5 :\n",
    "#                         target_cls[anchor_idx] = _label\n",
    "#                         target_loc[anchor_idx] = get_loc_offset((_x1, _y1, _x2, _y2), anchorbox[anchor_idx])\n",
    "#                         target_iou[anchor_idx] = iou\n",
    "#                     elif iou >0.4:\n",
    "#                         target_cls[anchor_idx] = -2 #ignore this example\n",
    "#                         target_iou[anchor_idx] = iou\n",
    "#                     else:\n",
    "#                         target_cls[anchor_idx] = bg_index\n",
    "#                         target_iou[anchor_idx] = iou\n",
    "#             if best_iou > 0 and best_iou < 0.5: #if gt has no >0.5 iou with any anchor\n",
    "#                 target_cls[best_anchor_idx] = _label\n",
    "#                 target_loc[best_anchor_idx] = get_loc_offset((_x1, _y1, _x2, _y2), anchorbox[best_anchor_idx])\n",
    "#                 target_iou[best_anchor_idx] = 1.0  # \n",
    "#         return target_cls, target_loc\n",
    "        \n",
    "        \n",
    "    def get_target(self, anchorbox, label, x1, y1, x2, y2, num_classes=20):\n",
    "        bg_index = num_classes -1\n",
    "        query_box= np.zeros((0,4))\n",
    "        query_label = np.zeros((0))\n",
    "        for _x1, _y1, _x2, _y2,_label in zip(x1, y1, x2, y2, label):\n",
    "            query_box = np.append(query_box, np.array([[_x1,_y1,_x2,_y2]]), axis=0)\n",
    "            query_label = np.append(query_label, _label)\n",
    "        \n",
    "        overlap = compute_overlap(anchorbox.astype(np.float64), query_box.astype(np.float64))\n",
    "        argmax_overlaps_inds = np.argmax(overlap, axis=1)\n",
    "        max_overlaps = overlap[ np.arange(overlap.shape[0]) , argmax_overlaps_inds]\n",
    "        positive_index = (max_overlaps > 0.5)\n",
    "        ignore_index = (max_overlaps > 0.4)  & ~positive_index\n",
    "        negative_index = (max_overlaps <= 0.4)\n",
    "        \n",
    "        target_loc = get_loc_offset(query_box[ argmax_overlaps_inds, :], anchorbox)\n",
    "        target_cls = query_label[argmax_overlaps_inds]\n",
    "        target_cls[negative_index] = bg_index\n",
    "        target_cls[ignore_index] = -2 # ignore this example\n",
    "        \n",
    "        return target_cls, target_loc\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_anchors_core(self, base_size, ratios, scales):\n",
    "        num_anchors = len(ratios) * len(scales)\n",
    "        # initialize output anchors\n",
    "        anchors = np.zeros((num_anchors, 4))\n",
    "        # scale base_size\n",
    "        anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "        # compute areas of anchors\n",
    "        areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "        # correct for ratios\n",
    "        anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "        anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "        # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "        anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "        anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "        return anchors\n",
    "    \n",
    "    def shift(self, image_shape, stride, anchors):\n",
    "        shift_x = (np.arange(0, image_shape[1]) + 0.5) * stride\n",
    "        shift_y = (np.arange(0, image_shape[0]) + 0.5) * stride\n",
    "        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "        shifts = np.vstack((\n",
    "            shift_x.ravel(), shift_y.ravel(),\n",
    "            shift_x.ravel(), shift_y.ravel()\n",
    "        )).transpose()\n",
    "\n",
    "        A = anchors.shape[0]\n",
    "        K = shifts.shape[0]\n",
    "        all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "        all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "        return all_anchors\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample_array = np.zeros((0, 4))\n",
    "# sample_array\n",
    "# for i in range(2):\n",
    "#     sample_array = np.append(sample_array,np.array([[i,i,i,i]]), axis=0)\n",
    "# sample_array\n",
    "\n",
    "# sample_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaLoss(Loss):\n",
    "    def focal_loss(self, cls_gt_example, cls_pred_example, num_classes, alpha=0.25, gamma=2.0):\n",
    "        bg_index = num_classes - 1 \n",
    "        # cls_gt has shape [A], cls_pred is in [A, K]\n",
    "        # gather the objects and background, discard the rest\n",
    "        obj_idx = tf.where(tf.logical_and( tf.greater_equal(cls_gt_example, 0), tf.less(cls_gt_example, bg_index)))\n",
    "        obj_bg_idx = tf.where(tf.greater_equal(cls_gt_example, 0))\n",
    "        obj_bg_count = tf.cast(tf.shape(obj_bg_idx)[0], tf.float32)\n",
    "        obj_count = tf.cast(tf.maximum(tf.shape(obj_idx)[0],1), tf.float32)\n",
    "        cls_gt_example = tf.one_hot(cls_gt_example, num_classes)\n",
    "        cls_gt_example = tf.gather_nd(cls_gt_example, obj_bg_idx)\n",
    "        cls_pred_example = tf.gather_nd(cls_pred_example, obj_bg_idx)\n",
    "        cls_gt_example = tf.reshape(cls_gt_example, (-1, 1))\n",
    "        cls_pred_example = tf.reshape(cls_pred_example, (-1, 1))\n",
    "        # compute the focal weight on each selected anchor box\n",
    "        alpha_factor = tf.ones_like(cls_gt_example) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(cls_gt_example, 1), alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = tf.where(tf.equal(cls_gt_example, 1), 1 - cls_pred_example, cls_pred_example)\n",
    "        focal_weight = alpha_factor * focal_weight**gamma / obj_count\n",
    "        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(cls_gt_example, cls_pred_example, sample_weight=focal_weight)\n",
    "        return cls_loss, obj_idx\n",
    "\n",
    "    def smooth_l1(self, loc_gt_example, loc_pred_example, obj_idx):\n",
    "        # loc_gt anf loc_pred has shape [A, 4]\n",
    "        sigma= 3\n",
    "        sigma_squared = sigma ** 3\n",
    "        obj_count = tf.cast(tf.maximum(tf.shape(obj_idx)[0],1), tf.float32)\n",
    "        loc_gt = tf.gather_nd(loc_gt_example, obj_idx)\n",
    "        loc_pred = tf.gather_nd(loc_pred_example, obj_idx)\n",
    "        loc_gt = tf.reshape(loc_gt, (-1, 1))\n",
    "        loc_pred = tf.reshape(loc_pred, (-1, 1))\n",
    "        loc_diff = tf.abs(loc_gt - loc_pred)\n",
    "        smooth_l1_loss = tf.where(tf.less(loc_diff, 1/sigma_squared), 0.5 * loc_diff**2 * sigma_squared, \n",
    "                                  loc_diff - 0.5/sigma_squared)\n",
    "        smooth_l1_loss = tf.reduce_sum(smooth_l1_loss)/ obj_count\n",
    "        return smooth_l1_loss\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        cls_gt, loc_gt, cls_pred, loc_pred = data\n",
    "        cls_gt = tf.cast(cls_gt, tf.int32 )\n",
    "        batch_size = state[\"batch_size_per_device\"]\n",
    "        total_loss = []\n",
    "        for idx in range(batch_size):\n",
    "            cls_gt_example = cls_gt[idx]\n",
    "            loc_gt_example = loc_gt[idx]\n",
    "            cls_pred_example = cls_pred[idx]\n",
    "            loc_pred_example = loc_pred[idx]\n",
    "            focal_loss, obj_idx = self.focal_loss(cls_gt_example, cls_pred_example, num_classes=80+1)\n",
    "            smooth_l1_loss = self.smooth_l1(loc_gt_example, loc_pred_example, obj_idx)\n",
    "            total_loss.append(focal_loss + smooth_l1_loss)\n",
    "        total_loss = tf.convert_to_tensor(total_loss)\n",
    "        return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictBox(TensorOp):\n",
    "    def __init__(self, num_classes, inputs=None, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.num_classes = num_classes\n",
    "        self.bg_index = num_classes - 1\n",
    "                   \n",
    "    def forward(self, data, state):\n",
    "        cls_pred, loc_pred, loc_base = data\n",
    "        input_width = 1280\n",
    "        input_height = 800\n",
    "        top_n = 300\n",
    "        score_threshold = 0.05   \n",
    "        std = 0.2\n",
    "        mean = 0\n",
    "        # convert the residual prediction to absolute prediction in (x1, y1, x2, y2)\n",
    "        anchor_w_h = tf.tile(loc_base[:,:, 2:], [1, 1, 2]) - tf.tile(loc_base[:,:,:2], [1, 1, 2])\n",
    "        anchorbox =  loc_base\n",
    "        loc_pred_abs = tf.map_fn(lambda x: (x[0]*std+mean) * x[1] + x[2],\n",
    "                             elems=(loc_pred, anchor_w_h, anchorbox),\n",
    "                             dtype=tf.float32,\n",
    "                             back_prop=False)\n",
    "        x1 = tf.clip_by_value(loc_pred_abs[:, :, 0], 0, input_width)\n",
    "        y1 = tf.clip_by_value(loc_pred_abs[:, :, 1], 0, input_height)\n",
    "        x2 = tf.clip_by_value(loc_pred_abs[:, :, 2], 0, input_width)\n",
    "        y2 = tf.clip_by_value(loc_pred_abs[:, :, 3], 0, input_height)\n",
    "        loc_pred_abs = tf.stack([x1, y1, x2, y2] ,axis=2)\n",
    "        \n",
    "        num_batch, num_anchor, _ = loc_pred_abs.shape\n",
    "        cls_best_score = tf.reduce_max(cls_pred, axis=-1)\n",
    "        cls_best_class = tf.argmax(cls_pred, axis=-1)\n",
    "        \n",
    "        cls_best_score = tf.where(tf.not_equal(cls_best_class, self.bg_index), cls_best_score,0)\n",
    "        \n",
    "        # select top n anchor boxes to proceed\n",
    "        # Padded Nonmax suppression with threshold\n",
    "        selected_indices_padded = tf.map_fn(\n",
    "            lambda x: tf.image.non_max_suppression_padded(\n",
    "                x[0], x[1], top_n, pad_to_max_output_size=True, score_threshold=score_threshold).selected_indices,\n",
    "            (loc_pred_abs, cls_best_score),\n",
    "            dtype=tf.int32,\n",
    "            back_prop=False)\n",
    "        valid_outputs = tf.map_fn(\n",
    "            lambda x: tf.image.non_max_suppression_padded(\n",
    "                x[0], x[1], top_n, pad_to_max_output_size=True, score_threshold=score_threshold).valid_outputs,\n",
    "            (loc_pred_abs, cls_best_score),\n",
    "            dtype=tf.int32,\n",
    "            back_prop=False)\n",
    "        return loc_pred_abs, selected_indices_padded, valid_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/coco/'\n",
    "train_csv = os.path.join(path,'train_coco.csv')\n",
    "val_csv = os.path.join(path,'val_coco.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32*40, 32*25\n",
    "writer = RecordWriter(\n",
    "        train_data=train_csv,\n",
    "        save_dir = '/home/ubuntu/coco/tf_records',\n",
    "        validation_data=val_csv,\n",
    "        ops=[\n",
    "                ImageReader(inputs=\"image\", parent_path=path, outputs=\"image\"),\n",
    "                String2IntArray(inputs=[\"label\"], outputs=[\"label\"]),\n",
    "                String2FloatArray(inputs=[\"x1\", \"y1\", \"x2\", \"y2\"], outputs=[\"x1\", \"y1\", \"x2\", \"y2\"]),\n",
    "                ResnetPreprocess(inputs=\"image\", outputs=\"image\"), \n",
    "                ResizeCocoStyle((800, 1280), keep_ratio=True, inputs=[\"image\", \"x1\", \"y1\", \"x2\", \"y2\" ] , outputs=[\"image\",\"x1\",\"y1\",\"x2\",\"y2\"]),\n",
    "                GenerateTarget(inputs=[\"label\",\"x1\",\"y1\",\"x2\",\"y2\",\"image\"], outputs=[\"target_cls\",\"target_loc\",\"base_loc\"], input_shape=(800,1280,3))\n",
    "        ])\n",
    "\n",
    "pipeline = Pipeline(batch_size=2, data=writer, read_feature=[\"image\",\"image_id\", \"target_cls\", \"target_loc\",\"base_loc\"])\n",
    "# pipeline = Pipeline(batch_size=2, data=writer, read_feature=[\"image\",\"image_id\", \"x1\",\"y1\",\"x2\",\"y2\"], padded_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Saving tfrecord to /home/ubuntu/coco/tf_records\n",
      "FastEstimator: Converting Train TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 4.8%, Speed: 157.12 record/sec\n",
      "FastEstimator: Converting Train TFRecords 9.6%, Speed: 110.50 record/sec\n",
      "FastEstimator: Converting Train TFRecords 14.4%, Speed: 86.67 record/sec\n",
      "FastEstimator: Converting Train TFRecords 19.1%, Speed: 47.79 record/sec\n",
      "FastEstimator: Converting Train TFRecords 23.9%, Speed: 65.08 record/sec\n",
      "FastEstimator: Converting Train TFRecords 28.7%, Speed: 9.41 record/sec\n",
      "FastEstimator: Converting Train TFRecords 33.5%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 38.3%, Speed: 6.81 record/sec\n",
      "FastEstimator: Converting Train TFRecords 43.1%, Speed: 6.90 record/sec\n",
      "FastEstimator: Converting Train TFRecords 47.9%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Train TFRecords 52.7%, Speed: 6.93 record/sec\n",
      "FastEstimator: Converting Train TFRecords 57.4%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Train TFRecords 62.2%, Speed: 6.90 record/sec\n",
      "FastEstimator: Converting Train TFRecords 67.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 71.8%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Train TFRecords 76.6%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Train TFRecords 81.4%, Speed: 6.88 record/sec\n",
      "FastEstimator: Converting Train TFRecords 86.2%, Speed: 6.94 record/sec\n",
      "FastEstimator: Converting Train TFRecords 91.0%, Speed: 6.90 record/sec\n",
      "FastEstimator: Converting Train TFRecords 95.7%, Speed: 7.01 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 4.3%, Speed: 76.76 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 8.5%, Speed: 78.55 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 12.8%, Speed: 16.71 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 17.0%, Speed: 6.16 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 21.3%, Speed: 6.45 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 25.5%, Speed: 6.77 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 29.8%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 34.0%, Speed: 6.80 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 38.3%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 42.6%, Speed: 6.88 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 46.8%, Speed: 6.87 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 51.1%, Speed: 6.87 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 55.3%, Speed: 6.87 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 59.6%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 63.8%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 68.1%, Speed: 6.93 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 72.3%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 76.6%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 80.9%, Speed: 6.92 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 85.1%, Speed: 6.88 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 89.4%, Speed: 6.88 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 93.6%, Speed: 6.89 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 97.9%, Speed: 6.89 record/sec\n",
      "FastEstimator: Reading non-empty directory: /home/ubuntu/coco/tf_records\n",
      "FastEstimator: Found 6000 examples for train in /home/ubuntu/coco/tf_records/train_summary0.json\n",
      "FastEstimator: Found 3000 examples for eval in /home/ubuntu/coco/tf_records/eval_summary0.json\n"
     ]
    }
   ],
   "source": [
    "show_batch = pipeline.show_results(mode='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,1,2,2],[2,2,3,3],[3,3,4,4],[4,4,5,5]])\n",
    "b = np.array([2,2,2,2])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.tile(a[:,2:]-a[:,:2],[1,2])\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=475, shape=(8, 191970), dtype=float32, numpy=\n",
       "array([[80., 80., 80., ..., 80., 80., 80.],\n",
       "       [80., 80., 80., ..., 80., 80., 80.],\n",
       "       [80., 80., 80., ..., 80., 80., 80.],\n",
       "       ...,\n",
       "       [80., 80., 80., ..., 80., 80., 80.],\n",
       "       [80., 80., 80., ..., 80., 80., 80.],\n",
       "       [80., 80., 80., ..., 80., 80., 80.]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_batch[0]['target_cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=476, shape=(8, 191970, 4), dtype=float32, numpy=\n",
       "array([[[ 7.44260178e+01,  8.07973480e+01,  7.71599960e+01,\n",
       "          7.94801941e+01],\n",
       "        [ 5.95877190e+01,  6.46446457e+01,  6.07261810e+01,\n",
       "          6.25677223e+01],\n",
       "        [ 4.78105507e+01,  5.18242378e+01,  4.76826553e+01,\n",
       "          4.91442833e+01],\n",
       "        ...,\n",
       "        [-5.24779129e+00, -7.70829201e-01, -9.28104401e+00,\n",
       "         -5.65574026e+00],\n",
       "        [-3.64942598e+00, -9.60588679e-02, -7.88211775e+00,\n",
       "         -5.00471258e+00],\n",
       "        [-2.38080263e+00,  4.39506710e-01, -6.77178955e+00,\n",
       "         -4.48799181e+00]],\n",
       "\n",
       "       [[ 6.07067528e+01,  1.09068619e+02,  9.42762146e+01,\n",
       "          1.53462112e+02],\n",
       "        [ 4.86987305e+01,  8.70835648e+01,  7.43113327e+01,\n",
       "          1.21287209e+02],\n",
       "        [ 3.91679573e+01,  6.96340256e+01,  5.84651947e+01,\n",
       "          9.57499695e+01],\n",
       "        ...,\n",
       "        [-6.96269941e+00,  1.12648062e-01, -7.14151669e+00,\n",
       "         -3.34380531e+00],\n",
       "        [-5.01054955e+00,  6.05157495e-01, -6.18397427e+00,\n",
       "         -3.16972876e+00],\n",
       "        [-3.46112704e+00,  9.96062517e-01, -5.42397213e+00,\n",
       "         -3.03156400e+00]],\n",
       "\n",
       "       [[ 3.00110359e+01,  2.35579548e+01,  1.24813515e+02,\n",
       "          1.72978821e+02],\n",
       "        [ 2.43355236e+01,  1.92137089e+01,  9.85488052e+01,\n",
       "          1.36777634e+02],\n",
       "        [ 1.98308659e+01,  1.57656803e+01,  7.77024918e+01,\n",
       "          1.08044724e+02],\n",
       "        ...,\n",
       "        [-1.07996635e+01, -2.55956006e+00, -3.32435393e+00,\n",
       "         -2.73390818e+00],\n",
       "        [-8.05595016e+00, -1.51577556e+00, -3.15429020e+00,\n",
       "         -2.68565321e+00],\n",
       "        [-5.87826347e+00, -6.87323153e-01, -3.01931047e+00,\n",
       "         -2.64735293e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.45109978e+01,  4.35541306e+01,  6.76360703e+01,\n",
       "          1.31454803e+02],\n",
       "        [ 3.58441505e+01,  3.50846863e+01,  5.31670380e+01,\n",
       "          1.03819992e+02],\n",
       "        [ 2.89652710e+01,  2.83624821e+01,  4.16829567e+01,\n",
       "          8.18862305e+01],\n",
       "        ...,\n",
       "        [ 1.33580267e-01, -1.93467963e+00, -2.14677835e+00,\n",
       "         -4.01494455e+00],\n",
       "        [ 6.21771395e-01, -1.01980758e+00, -2.21964765e+00,\n",
       "         -3.70241213e+00],\n",
       "        [ 1.00924897e+00, -2.93673098e-01, -2.27748418e+00,\n",
       "         -3.45435524e+00]],\n",
       "\n",
       "       [[ 1.38431711e+01,  1.98440135e+00,  1.26694298e+02,\n",
       "          1.67868546e+02],\n",
       "        [ 1.15030813e+01,  2.09076905e+00,  1.00041588e+02,\n",
       "          1.32721603e+02],\n",
       "        [ 9.64575005e+00,  2.17519307e+00,  7.88873062e+01,\n",
       "          1.04825455e+02],\n",
       "        ...,\n",
       "        [-1.28206472e+01, -3.23373365e+00, -3.08925557e+00,\n",
       "         -2.89360428e+00],\n",
       "        [-9.66000557e+00, -2.05086756e+00, -2.96769261e+00,\n",
       "         -2.81240392e+00],\n",
       "        [-7.15140295e+00, -1.11202586e+00, -2.87120771e+00,\n",
       "         -2.74795532e+00]],\n",
       "\n",
       "       [[ 1.06622894e+02,  6.78556290e+01,  1.31853531e+02,\n",
       "          1.06325310e+02],\n",
       "        [ 8.51423950e+01,  5.43727951e+01,  1.04136459e+02,\n",
       "          8.38747025e+01],\n",
       "        [ 6.80933151e+01,  4.36714668e+01,  8.21374130e+01,\n",
       "          6.60556488e+01],\n",
       "        ...,\n",
       "        [-1.22318125e+00, -1.17525792e+00, -2.44435287e+00,\n",
       "         -4.81683064e+00],\n",
       "        [-4.55090880e-01, -4.17054117e-01, -2.45583296e+00,\n",
       "         -4.33886957e+00],\n",
       "        [ 1.54542819e-01,  1.84732616e-01, -2.46494460e+00,\n",
       "         -3.95951176e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_batch[0]['target_loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(show_batch))\n",
    "# show_batch[0].keys()\n",
    "\n",
    "# show_batch[0]['base_loc'][0]\n",
    "\n",
    "# batch_idx=0\n",
    "# high = len(show_batch[batch_idx]['image_id'])\n",
    "# idx = np.random.randint(low=0,high=high)\n",
    "# print('selected index', idx)\n",
    "# image = show_batch[batch_idx]['image'][idx]\n",
    "# target_cls = show_batch[batch_idx]['target_cls'][idx]\n",
    "# target_loc = show_batch[batch_idx]['target_loc'][idx]\n",
    "# base_loc = show_batch[batch_idx]['base_loc'][idx]\n",
    "\n",
    "# bg_index = 80\n",
    "# obj_idx = tf.where(tf.logical_and( tf.greater(target_cls, 0), tf.less(target_cls, bg_index)))\n",
    "# # obj_bg_idx = tf.where(tf.greater_equal(target_cls, 0))\n",
    "# target_cls_filt = tf.gather_nd(target_cls, obj_idx)\n",
    "# target_loc_filt = tf.gather_nd(target_loc, obj_idx)\n",
    "\n",
    "# print(target_loc_filt)\n",
    "# print(target_cls_filt)\n",
    "# print(base_loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=7.86s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "model = FEModel(model_def=lambda: RetinaNet(input_shape=(800, 1280, 3), num_classes=80+1),\n",
    "                model_name=\"retinanet\",\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.00001))\n",
    "network = Network(ops=[\n",
    "    ModelOp(inputs=\"image\", model=model, outputs=[\"pred_cls\", \"pred_loc\"]),\n",
    "    PredictBox(80+1, inputs=[\"pred_cls\",\"pred_loc\",\"base_loc\"], outputs=(\"abs_loc\",\"selected_indices_padded\", \"valid_outputs\"), mode=\"eval\"),\n",
    "    RetinaLoss(inputs=(\"target_cls\", \"target_loc\", \"pred_cls\", \"pred_loc\"), outputs=\"loss\"),\n",
    "])\n",
    "# prepare estimator\n",
    "model_dir = '/home/ubuntu/coco/bestmodel'\n",
    "traces = [MeanAveragePrecision('selected_indices_padded','valid_outputs','image_id','pred_cls', 'abs_loc',\n",
    "                                coco_path='/home/ubuntu/coco', val_csv='val_coco.csv'), \n",
    "            LRController(model_name=\"retinanet\", reduce_on_eval=True, reduce_patience=2, min_lr=1e-09,  reduce_factor=0.33),\n",
    "            ModelSaver(model_name=\"retinanet\", save_dir=model_dir, save_best=True)\n",
    "         ]\n",
    "estimator = Estimator(network=network, pipeline=pipeline, epochs=100, log_steps=10, traces=traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator: Reading non-empty directory: /home/ubuntu/coco/tf_records\n",
      "FastEstimator: Found 6000 examples for train in /home/ubuntu/coco/tf_records/train_summary0.json\n",
      "FastEstimator: Found 3000 examples for eval in /home/ubuntu/coco/tf_records/eval_summary0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1021 20:46:05.785572 140684334917440 mirrored_strategy.py:659] Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "W1021 20:46:10.868411 140684334917440 mirrored_strategy.py:659] Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Start: step: 0; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 0; loss: 6232.865; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 10; loss: 3099.5024; examples/sec: 19.77; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 20; loss: 2249.901; examples/sec: 5.29; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 30; loss: 2990.9507; examples/sec: 4.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 40; loss: 1003.57153; examples/sec: 4.56; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 50; loss: 422.76025; examples/sec: 4.71; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 60; loss: 213.85661; examples/sec: 5.03; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 70; loss: 96.674774; examples/sec: 5.03; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 80; loss: 24.450075; examples/sec: 5.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 90; loss: 25.320282; examples/sec: 5.12; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 100; loss: 20.067902; examples/sec: 5.38; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 110; loss: 41.25068; examples/sec: 4.95; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 120; loss: 14.460293; examples/sec: 4.89; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 130; loss: 16.34402; examples/sec: 5.18; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 140; loss: 13.40526; examples/sec: 4.79; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 150; loss: 17.23304; examples/sec: 5.0; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 160; loss: 13.497117; examples/sec: 5.57; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 170; loss: 14.194377; examples/sec: 5.68; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 180; loss: 13.550327; examples/sec: 5.38; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 190; loss: 13.085945; examples/sec: 6.42; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 200; loss: 13.884592; examples/sec: 5.78; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 210; loss: 14.342697; examples/sec: 5.33; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 220; loss: 13.882274; examples/sec: 5.19; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 230; loss: 12.564884; examples/sec: 4.66; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 240; loss: 12.589312; examples/sec: 4.81; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 250; loss: 11.34672; examples/sec: 4.71; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 260; loss: 11.766364; examples/sec: 4.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 270; loss: 10.525147; examples/sec: 4.18; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 280; loss: 14.830892; examples/sec: 4.34; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 290; loss: 12.478989; examples/sec: 4.45; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 300; loss: 13.174278; examples/sec: 4.49; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 310; loss: 11.635641; examples/sec: 4.45; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 320; loss: 11.47339; examples/sec: 4.44; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 330; loss: 10.464094; examples/sec: 4.44; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 340; loss: 11.912092; examples/sec: 4.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 350; loss: 10.469259; examples/sec: 4.26; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 360; loss: 11.726008; examples/sec: 4.35; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 370; loss: 10.507474; examples/sec: 4.44; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 380; loss: 11.250339; examples/sec: 4.14; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 390; loss: 9.006924; examples/sec: 4.17; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 400; loss: 10.58526; examples/sec: 4.15; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 410; loss: 9.613222; examples/sec: 4.23; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 420; loss: 11.810556; examples/sec: 4.09; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 430; loss: 9.275791; examples/sec: 4.29; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 440; loss: 12.06488; examples/sec: 4.34; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 450; loss: 8.644142; examples/sec: 4.3; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 460; loss: 10.302748; examples/sec: 4.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 470; loss: 9.053777; examples/sec: 4.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 480; loss: 10.226936; examples/sec: 4.45; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 490; loss: 9.838449; examples/sec: 4.19; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 500; loss: 9.568785; examples/sec: 4.38; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 510; loss: 10.730193; examples/sec: 4.24; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 520; loss: 9.0775795; examples/sec: 4.21; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 530; loss: 8.559733; examples/sec: 4.3; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 540; loss: 8.276941; examples/sec: 4.22; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 550; loss: 9.421303; examples/sec: 4.3; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 560; loss: 10.66056; examples/sec: 4.32; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 570; loss: 8.82542; examples/sec: 4.22; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 580; loss: 8.887123; examples/sec: 4.15; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 590; loss: 9.937893; examples/sec: 4.12; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 600; loss: 9.020306; examples/sec: 3.92; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 610; loss: 10.675797; examples/sec: 4.16; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 620; loss: 8.336414; examples/sec: 4.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 630; loss: 8.705371; examples/sec: 4.22; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 640; loss: 10.662924; examples/sec: 4.46; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 650; loss: 7.4359045; examples/sec: 4.39; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 660; loss: 9.632891; examples/sec: 4.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 670; loss: 8.519173; examples/sec: 4.29; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 680; loss: 9.42525; examples/sec: 4.28; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 690; loss: 8.431476; examples/sec: 4.37; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 700; loss: 8.624628; examples/sec: 4.19; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 710; loss: 8.368095; examples/sec: 4.33; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 720; loss: 7.7454987; examples/sec: 3.85; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 730; loss: 7.302841; examples/sec: 4.02; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 740; loss: 6.68809; examples/sec: 4.09; retinanet_lr: 1e-05; \n",
      "Loading and preparing results...\n",
      "No records found to evaluate\n",
      "FastEstimator-ModelSaver: Saving model to /home/ubuntu/coco/bestmodel/retinanet_best_loss.h5\n",
      "FastEstimator-Eval: step: 750; epoch: 0; loss: 8.157999; min_loss: 8.157999; since_best_loss: 0; \n",
      "FastEstimator-Train: step: 750; loss: 7.788988; examples/sec: 10.45; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 760; loss: 8.192326; examples/sec: 20.43; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 770; loss: 8.085354; examples/sec: 10.52; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 780; loss: 6.7370105; examples/sec: 6.61; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 790; loss: 7.577298; examples/sec: 7.21; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 800; loss: 7.8643484; examples/sec: 6.26; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 810; loss: 8.055985; examples/sec: 5.21; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 820; loss: 7.3741074; examples/sec: 6.03; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 830; loss: 7.5018415; examples/sec: 5.26; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 840; loss: 7.148465; examples/sec: 6.49; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 850; loss: 9.222324; examples/sec: 5.99; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 860; loss: 6.388357; examples/sec: 6.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 870; loss: 7.559874; examples/sec: 5.68; retinanet_lr: 1e-05; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 880; loss: 7.021303; examples/sec: 6.42; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 890; loss: 7.0262327; examples/sec: 6.26; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 900; loss: 8.055449; examples/sec: 6.0; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 910; loss: 6.1037555; examples/sec: 6.53; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 920; loss: 7.324296; examples/sec: 6.06; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 930; loss: 7.1842594; examples/sec: 4.98; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 940; loss: 6.5925837; examples/sec: 4.92; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 950; loss: 7.0213814; examples/sec: 5.05; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 960; loss: 6.0299788; examples/sec: 4.59; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 970; loss: 6.3313217; examples/sec: 4.66; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 980; loss: 7.0540633; examples/sec: 4.61; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 990; loss: 6.1628304; examples/sec: 4.65; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1000; loss: 6.5725465; examples/sec: 4.44; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1010; loss: 6.9465475; examples/sec: 4.61; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1020; loss: 6.6161337; examples/sec: 4.74; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1030; loss: 7.1079845; examples/sec: 4.16; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1040; loss: 6.8514667; examples/sec: 4.09; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1050; loss: 6.447752; examples/sec: 4.37; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1060; loss: 6.573583; examples/sec: 4.35; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1070; loss: 6.0408025; examples/sec: 4.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1080; loss: 6.2190065; examples/sec: 4.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1090; loss: 6.0748262; examples/sec: 4.23; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1100; loss: 6.2992153; examples/sec: 4.23; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1110; loss: 6.7378163; examples/sec: 4.21; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1120; loss: 5.902398; examples/sec: 4.08; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1130; loss: 5.778885; examples/sec: 4.11; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1140; loss: 5.4918423; examples/sec: 4.37; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1150; loss: 6.1134434; examples/sec: 4.12; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1160; loss: 5.563607; examples/sec: 4.3; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1170; loss: 6.0286374; examples/sec: 3.98; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1180; loss: 5.83893; examples/sec: 4.26; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1190; loss: 6.2045455; examples/sec: 4.2; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1200; loss: 6.0689554; examples/sec: 4.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1210; loss: 5.869569; examples/sec: 4.3; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1220; loss: 5.6282225; examples/sec: 4.26; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1230; loss: 5.5849895; examples/sec: 4.35; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1240; loss: 6.120262; examples/sec: 4.31; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1250; loss: 6.0437536; examples/sec: 4.11; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1260; loss: 5.9834013; examples/sec: 4.3; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1270; loss: 5.445462; examples/sec: 4.36; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1280; loss: 6.5688663; examples/sec: 4.35; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1290; loss: 5.5518723; examples/sec: 4.24; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1300; loss: 5.612105; examples/sec: 4.35; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1310; loss: 5.4706697; examples/sec: 4.35; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1320; loss: 5.4422617; examples/sec: 4.42; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1330; loss: 5.355648; examples/sec: 4.28; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1340; loss: 5.5446334; examples/sec: 4.42; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1350; loss: 5.4457192; examples/sec: 4.42; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1360; loss: 5.356029; examples/sec: 4.44; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1370; loss: 5.277893; examples/sec: 4.44; retinanet_lr: 1e-05; \n",
      "FastEstimator-Train: step: 1380; loss: 5.3760157; examples/sec: 4.42; retinanet_lr: 1e-05; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastesti2",
   "language": "python",
   "name": "fastesti2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
